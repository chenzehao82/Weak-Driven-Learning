import argparse
import os
import torch
import json
import datetime
import re

from trl import SFTTrainer, SFTConfig
from datasets import Dataset
from transformers import AutoTokenizer
from accelerate import PartialState

from utils.utils import load_model_tokenizer  # ä½ åŸæ¥çš„å‡½æ•°
# ================== åˆ†å¸ƒå¼è°ƒè¯•ä¿¡æ¯ï¼ˆä¿ç•™ï¼‰ ==================
if os.environ.get("RANK", None) is not None:
    rank = os.environ["RANK"]
    local_rank = os.environ.get("LOCAL_RANK", "?")
    print(f"[rank {rank}] LOCAL_RANK={local_rank}, CUDA_VISIBLE_DEVICES={os.environ.get('CUDA_VISIBLE_DEVICES')}")

# ================== checkpoint å·¥å…·å‡½æ•° ==================
PREFIX_CHECKPOINT_DIR = "checkpoint"
_re_checkpoint = re.compile(r"^" + PREFIX_CHECKPOINT_DIR + r"\-(\d+)$")

def get_last_checkpoint(folder):
    if not os.path.isdir(folder):
        return None
    content = os.listdir(folder)
    checkpoints = [
        path
        for path in content
        if _re_checkpoint.search(path) is not None and os.path.isdir(os.path.join(folder, path))
    ]
    if len(checkpoints) == 0:
        return None
    return os.path.join(folder, max(checkpoints, key=lambda x: int(_re_checkpoint.search(x).groups()[0])))

# ================== DeepSpeed çŠ¶æ€æ£€æŸ¥ï¼ˆä¿ç•™ï¼‰ ==================
def check_deepspeed_status(trainer):
    """æ”¹è¿›çš„DeepSpeedçŠ¶æ€æ£€æŸ¥"""
    try:
        # æ–¹æ³•1: æ£€æŸ¥traineræ˜¯å¦ä½¿ç”¨äº†deepspeed
        if hasattr(trainer, 'is_deepspeed_enabled') and trainer.is_deepspeed_enabled:
            print("âœ“ DeepSpeedå·²å¯ç”¨")
            return True
            
        # æ–¹æ³•2: æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¢«DeepSpeedåŒ…è£…
        model = trainer.model
        if hasattr(model, 'module') and hasattr(model.module, 'engine'):
            engine = model.module.engine
            print("=== DeepSpeedçŠ¶æ€ ===")
            print(f"ZeROé˜¶æ®µ: {engine.zero_optimization_stage()}")
            print(f"ä¼˜åŒ–å™¨: {type(engine.optimizer).__name__}")
            return True
            
        # æ–¹æ³•3: ç›´æ¥æ£€æŸ¥accelerateçŠ¶æ€
        from accelerate.utils import is_deepspeed_available
        if is_deepspeed_available():
            try:
                from deepspeed import comm as dist
                if dist.is_initialized():
                    print("âœ“ DeepSpeedåˆ†å¸ƒå¼å·²åˆå§‹åŒ–")
                    return True
            except:
                pass
        
        print("âœ— DeepSpeedæœªæ­£ç¡®åˆå§‹åŒ–")
        return False
    except Exception as e:
        print(f"âœ— æ£€æŸ¥DeepSpeedçŠ¶æ€å¤±è´¥: {e}")
        return False

# ================== é€šç”¨ SFT å‡½æ•°ï¼ˆä½ éœ€è¦çš„æ ¸å¿ƒå°è£…ï¼‰ ==================
def run_sft(
    model,
    tokenizer,
    train_dataset,
    *,
    output_dir: str,
    max_seq_length: int = 4096,
    per_device_train_batch_size: int = 1,
    grad_accum: int = 8,
    num_epochs: int = 1,
    learning_rate: float = 1e-5,
    bf16: bool = True,
    wandb_run_name: str | None = None,
    logging_steps: int = 1,
    seed: int = 42,
    resume_from_checkpoint: bool = True,  # æ˜¯å¦ä» checkpoint æ¢å¤
):
    """
    å¯¹ç»™å®š model / tokenizer / dataset è¿›è¡Œ SFT å¾®è°ƒã€‚
    
    å‚æ•°:
        model, tokenizer: å·²åŠ è½½å¥½çš„æ¨¡å‹ä¸åˆ†è¯å™¨
        train_dataset: HF Dataset æˆ– IterableDataset
        output_dir: checkpoint è¾“å‡ºç›®å½•
        å…¶ä»–å‚æ•°ä¸ä½ åŸæ¥çš„ argparse å¯¹åº”
    """
    distributed_state = PartialState()

    # ------ å¤„ç†æ•°æ®é›† & è®¡ç®— max_steps ------
    if isinstance(train_dataset, Dataset):
        # Map-style Datasetï¼šèµ°ä½ åŸæ¥çš„é€»è¾‘
        dataset = train_dataset.shuffle(seed=seed)
        max_step = (
            len(dataset)
            * num_epochs
            // (
                distributed_state.num_processes
                * per_device_train_batch_size
                * grad_accum
            )
        )
        dataset = dataset.shuffle(seed=seed).to_iterable_dataset(
            num_shards=distributed_state.num_processes * 2
        )
    else:
        # å·²ç»æ˜¯ IterableDataset çš„æƒ…å†µï¼šä¸å¼ºè¡Œæ”¹ï¼Œåªç»™ä¸ª None
        dataset = train_dataset
        max_step = None

    # ------ ä¸»è¿›ç¨‹æ‰“å°ä¿¡æ¯ & å»ºç›®å½• ------
    if distributed_state.is_main_process:
        print("DeepSpeed config:", os.environ.get("DEEPSPEED_CONFIG_FILE", "Not set"))
        os.makedirs(output_dir, exist_ok=True)

        print("\n=========== SFT é…ç½®å‚æ•° ===========")
        print(f"output_dir: {output_dir}")
        print(f"max_seq_length: {max_seq_length}")
        print(f"per_device_train_batch_size: {per_device_train_batch_size}")
        print(f"grad_accum: {grad_accum}")
        print(f"num_epochs: {num_epochs}")
        print(f"learning_rate: {learning_rate}")
        print(f"bf16: {bf16}")
        print(f"wandb_run_name: {wandb_run_name}")
        print("====================================\n")

        print(f"æ•°æ®é›†æ ·æœ¬æ•°(ä¼°ç®—): {len(train_dataset) if isinstance(train_dataset, Dataset) else 'IterableDataset'}")

    # åŒæ­¥
    distributed_state.wait_for_everyone()

    # ------ checkpoint æ¢å¤ ------
    if resume_from_checkpoint:
        checkpoint = get_last_checkpoint(output_dir)
        if checkpoint and distributed_state.is_main_process:
            print(f"ğŸ“‚ æ‰¾åˆ° checkpoint: {checkpoint}ï¼Œå°†å°è¯•æ¢å¤è®­ç»ƒ")
    else:
        checkpoint = None
        if distributed_state.is_main_process:
            print("âš ï¸ resume_from_checkpoint=Falseï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")

    # ------ æ„é€  SFTConfig ------
    logging_dir = os.path.join(
        "tensorboard_logs",
        (wandb_run_name or "run") + "_" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S"),
    )

    sft_config = SFTConfig(
        max_length=max_seq_length,
        per_device_train_batch_size=per_device_train_batch_size,
        gradient_accumulation_steps=grad_accum,
        num_train_epochs=num_epochs,
        learning_rate=learning_rate,
        bf16=bf16,
        logging_steps=logging_steps,
        save_strategy="epoch",
        # save_strategy="steps",        # å°†ä¿å­˜ç­–ç•¥ä» "epoch" æ”¹ä¸º "steps"
        # save_steps=100,               # è®¾ç½®æ¯ 100 æ­¥ä¿å­˜ä¸€æ¬¡
        output_dir=output_dir,
        report_to="tensorboard",
        logging_dir=logging_dir,
        run_name=wandb_run_name,
        gradient_checkpointing=True,
        max_steps=max_step,  # æ²¿ç”¨ä½ åŸæ¥çš„ max_step é€»è¾‘
        accelerator_config={"dispatch_batches": False},
        seed=seed,
        save_only_model=True
    )

    # ------ åˆ›å»º Trainer ------
    trainer = SFTTrainer(
        model=model,
        processing_class=tokenizer,  # ä½¿ç”¨ tokenizer
        args=sft_config,
        train_dataset=dataset,
    )

    # ------ æ£€æŸ¥ DeepSpeed çŠ¶æ€ ------
    if check_deepspeed_status(trainer):
        print("âœ“ DeepSpeed æ­£å¸¸å·¥ä½œ")
    else:
        print("âš  ä½¿ç”¨é DeepSpeed æ¨¡å¼è®­ç»ƒ")

    # ------ å¼€å§‹è®­ç»ƒ ------
    if distributed_state.is_main_process:
        print("\n=========== å¼€å§‹è®­ç»ƒ ===========\n")

    trainer.train(resume_from_checkpoint=checkpoint)

    return trainer  # æ–¹ä¾¿åé¢æ‹¿ log / model ç­‰


# ================== åŸæ¥ main çš„ä¸€ä¸ªç®€å•å°è£…ç¤ºä¾‹ï¼ˆå¯é€‰ï¼‰ ==================
def parse_known_args():
    """åªè§£æå·²çŸ¥å‚æ•°ï¼Œå¿½ç•¥æœªçŸ¥å‚æ•°"""
    parser = argparse.ArgumentParser(description="Finetune Qwen3 model")
    
    parser.add_argument("--local_rank", type=int, default=-1,
                        help="Local rank for distributed training")
    
    parser.add_argument("--model-name", type=str, required=True)
    parser.add_argument("--max-seq-length", type=int, default=2048)
    parser.add_argument("--per-device-train-batch-size", type=int, default=1)
    parser.add_argument("--grad-accum", type=int, default=8)
    parser.add_argument("--num-epochs", type=int, default=1)
    parser.add_argument("--lr", type=float, default=1e-5)
    parser.add_argument("--bf16", type=bool, default=True)
    parser.add_argument("--data-files", nargs="+", required=True)
    parser.add_argument("--output-dir", type=str, default="./output")
    parser.add_argument("--wandb-project", type=str)
    parser.add_argument("--wandb-run-name", type=str)
    
    args, unknown = parser.parse_known_args()
    if unknown:
        print(f"å¿½ç•¥æœªçŸ¥å‚æ•°: {unknown}")
    return args

def load_jsonl_files(data_files):
    """ç®€å•çš„ JSONL åŠ è½½å‡½æ•°ï¼ˆä»ä½ åŸæ¥çš„ main ä¸­æŠ½å‡ºæ¥ï¼‰"""
    all_records = []
    print("æ­£åœ¨ä»æœ¬åœ° JSONL æ–‡ä»¶åŠ è½½æ•°æ®...")
    for file_path in data_files:
        print(f"åŠ è½½æ–‡ä»¶ï¼š{file_path}")
        with open(file_path, "r", encoding="utf-8") as fin:
            for line in fin:
                line = line.strip()
                if not line:
                    continue
                try:
                    all_records.append(json.loads(line))
                except Exception as e:
                    print(f"[è­¦å‘Š] è·³è¿‡åè¡Œ({file_path}): {e}")
    print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…±è¯»å–æ ·æœ¬æ•°ï¼š{len(all_records)}")
    return Dataset.from_list(all_records)

def main():
    args = parse_known_args()

    # 1) åŠ è½½æ•°æ®
    dataset = load_jsonl_files(args.data_files)

    # 2) åŠ è½½æ¨¡å‹å’Œ tokenizerï¼ˆä½ çš„å·¥å…·å‡½æ•°ï¼‰
    model, tokenizer = load_model_tokenizer(args.model_name)

    # 3) è°ƒç”¨ç»Ÿä¸€çš„ run_sft è¿›è¡Œå¾®è°ƒ
    run_sft(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        output_dir=args.output_dir,
        max_seq_length=args.max_seq_length,
        per_device_train_batch_size=args.per_device_train_batch_size,
        grad_accum=args.grad_accum,
        num_epochs=args.num_epochs,
        learning_rate=args.lr,
        bf16=args.bf16,
        wandb_run_name=args.wandb_run_name,
    )

if __name__ == "__main__":
    main()
