import os
import json
import sys
import torch
import re
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig

# Handle execution both as package (`EnsembleLLM.utils`) and as top-level script (`utils`)
try:
    from EnsembleLLM.EnsembleQwen3.modeling_qwen3 import QwenBoostForCausalLM
except ImportError:
    try:
        from EnsembleQwen3.modeling_qwen3 import QwenBoostForCausalLM
    except ImportError as exc:
        _current_dir = os.path.dirname(os.path.abspath(__file__))
        _project_root = os.path.abspath(os.path.join(_current_dir, ".."))
        if _project_root not in sys.path:
            sys.path.insert(0, _project_root)
        try:
            from EnsembleQwen3.modeling_qwen3 import QwenBoostForCausalLM
        except ImportError:
            raise ImportError("Cannot locate `Qwen3ForEnsemble`. Please check PYTHONPATH.") from exc

def extract_submodel(ensemble_model_path, submodel_idx, save_dir, torch_dtype=torch.bfloat16):
    """
    ä»èåˆæ¨¡å‹ä¸­æå–æŒ‡å®šçš„å­æ¨¡å‹å¹¶ä¿å­˜ä¸ºç‹¬ç«‹æ¨¡å‹ã€‚

    Args:
        ensemble_model_path (str): èåˆæ¨¡å‹è·¯å¾„
        submodel_idx (int): è¦æå–çš„å­æ¨¡å‹ç´¢å¼•ï¼ˆ0è¡¨ç¤ºç¬¬ä¸€ä¸ªï¼Œ1è¡¨ç¤ºç¬¬äºŒä¸ªï¼‰
        save_dir (str): ä¿å­˜çš„ç›®å½•
        torch_dtype: ä¿å­˜çš„æƒé‡ç²¾åº¦ (é»˜è®¤ bfloat16)

    Returns:
        str: ä¿å­˜è·¯å¾„
    """
    print(f"ğŸ”¹ ä»èåˆæ¨¡å‹ä¸­æå–å­æ¨¡å‹:")
    print(f"   - èåˆæ¨¡å‹: {ensemble_model_path}")
    print(f"   - å­æ¨¡å‹ç´¢å¼•: {submodel_idx}")
    print(f"   - ä¿å­˜åˆ°: {save_dir}")

    os.makedirs(save_dir, exist_ok=True)

    # åŠ è½½èåˆæ¨¡å‹
    print("\nğŸ”¹ åŠ è½½èåˆæ¨¡å‹...")
    ensemble_model = QwenBoostForCausalLM.from_pretrained(
        ensemble_model_path,
        torch_dtype="auto",
        device_map=None,
        attn_implementation="flash_attention_2"
    )
    ensemble_state_dict = ensemble_model.state_dict()

    # æå–å­æ¨¡å‹çš„æƒé‡
    submodel_prefix = f"sub_models.{submodel_idx}."
    extracted_state_dict = {}
    
    print(f"\nğŸ”¹ æå– {submodel_prefix} çš„æƒé‡...")
    for key, value in ensemble_state_dict.items():
        if key.startswith(submodel_prefix):
            # ç§»é™¤ sub_models.{idx}. å‰ç¼€
            new_key = key[len(submodel_prefix):]
            # è·³è¿‡ prev_*_proj æƒé‡ï¼ˆè¿™äº›æ˜¯ç”¨äºgateçš„ï¼Œä¸å±äºåŸå§‹æ¨¡å‹ï¼‰
            if "prev_" not in new_key:
                extracted_state_dict[new_key] = value.to(torch_dtype)
    
    print(f"   âœ… æå–äº† {len(extracted_state_dict)} ä¸ªæƒé‡å¼ é‡")

    # ä¿å­˜ tokenizer
    print("\nğŸ”¹ ä¿å­˜ tokenizer ...")
    tokenizer = AutoTokenizer.from_pretrained(ensemble_model_path)
    tokenizer.save_pretrained(save_dir)

    # åŠ è½½åŸºç¡€æ¨¡å‹ç»“æ„ï¼ˆç”¨äºä¿å­˜é…ç½®ï¼‰
    print("\nğŸ”¹ ä¿å­˜ config.json ...")
    base_config = AutoConfig.from_pretrained(ensemble_model_path)
    # ä»èåˆæ¨¡å‹çš„é…ç½®ä¸­è·å–å­æ¨¡å‹é…ç½®
    if hasattr(base_config, "sub_model_cfgs") and base_config.sub_model_cfgs:
        # å¤åˆ¶å­æ¨¡å‹é…ç½®å­—å…¸ï¼ˆé¿å…ä¿®æ”¹åŸå§‹é…ç½®ï¼‰
        submodel_config_dict = dict(base_config.sub_model_cfgs[submodel_idx])
        # ç§»é™¤èåˆç›¸å…³çš„é…ç½®å­—æ®µ
        submodel_config_dict.pop("num_sub_models", None)
        submodel_config_dict.pop("is_llmboost", None)
        submodel_config_dict.pop("sub_model_cfgs", None)
        # æ¢å¤ä¸ºæ ‡å‡†çš„ Qwen3 é…ç½®
        if "architectures" not in submodel_config_dict or not submodel_config_dict.get("architectures"):
            submodel_config_dict["architectures"] = ["Qwen3ForCausalLM"]
    else:
        # å¦‚æœæ²¡æœ‰å­æ¨¡å‹é…ç½®ï¼Œä½¿ç”¨åŸºç¡€é…ç½®
        submodel_config_dict = base_config.to_dict()
        submodel_config_dict.pop("num_sub_models", None)
        submodel_config_dict.pop("is_llmboost", None)
        submodel_config_dict.pop("sub_model_cfgs", None)
        if "architectures" not in submodel_config_dict or not submodel_config_dict.get("architectures"):
            submodel_config_dict["architectures"] = ["Qwen3ForCausalLM"]

    with open(os.path.join(save_dir, "config.json"), "w") as f:
        json.dump(submodel_config_dict, f, indent=2, ensure_ascii=False)

    # ä¿å­˜æƒé‡
    print("\nğŸ”¹ ä¿å­˜æƒé‡...")
    final_path = os.path.join(save_dir, "pytorch_model.bin")
    torch.save(extracted_state_dict, final_path)

    print(f"\nğŸ‰ æå–å®Œæˆ! ä¿å­˜åˆ°: {final_path}")
    return final_path

def fuse_submodels(model_list, save_dir, torch_dtype=torch.bfloat16, fusion_lambda=0.5):
    """
    å°†å¤šä¸ª CausalLM æ¨¡å‹åˆå¹¶ä¸ºä¸€ä¸ª Ensemble æ¨¡å‹çš„æƒé‡æ ¼å¼ã€‚

    Args:
        model_list (List[str]): æ¨¡å‹è·¯å¾„åˆ—è¡¨æˆ– HF åç§°åˆ—è¡¨
        save_dir (str): ä¿å­˜çš„ç›®å½•
        torch_dtype: ä¿å­˜çš„æƒé‡ç²¾åº¦ (é»˜è®¤ bfloat16)
        fusion_lambda (float): èåˆæƒé‡ï¼Œå‰é¢çš„æ¨¡å‹å  (1-lambda)ï¼Œåé¢çš„æ¨¡å‹å  lambdaã€‚é»˜è®¤ 0.5 è¡¨ç¤ºå¹³å‡èåˆ

    Returns:
        str: ä¿å­˜è·¯å¾„
    """

    print(f"ğŸ”¹ Loading {len(model_list)} models:")
    for m in model_list:
        print(f"   - {m}")

    os.makedirs(save_dir, exist_ok=True)

    print("\nğŸ”¹ Saving tokenizer ...")
    tokenizer = AutoTokenizer.from_pretrained(model_list[0])
    tokenizer.save_pretrained(save_dir)

    print("\nğŸ”¹ Loading model weights ...")
    models = [
        AutoModelForCausalLM.from_pretrained(m, torch_dtype=torch_dtype)
        for m in model_list
    ]

    print("\nğŸ”¹ Saving config.json ...")
    config = AutoConfig.from_pretrained(model_list[0])
    config_dict = config.to_dict()

    # ç¡®ä¿ä¿å­˜ model_typeï¼Œè¿™å¯¹äºæ­£ç¡®åŠ è½½æ¨¡å‹å¾ˆé‡è¦
    if "model_type" not in config_dict:
        # å¦‚æœ config ä¸­æ²¡æœ‰ model_typeï¼Œå°è¯•ä»æ¨¡å‹è·¯å¾„æˆ–é…ç½®æ¨æ–­
        if hasattr(config, "model_type"):
            config_dict["model_type"] = config.model_type
        elif "qwen2" in model_list[0].lower():
            config_dict["model_type"] = "qwen2"
        elif "qwen3" in model_list[0].lower():
            config_dict["model_type"] = "qwen3"
        else:
            pass

    sub_model_cfgs = []
    for idx, m in enumerate(model_list):
        sub_cfg = AutoConfig.from_pretrained(m)
        sub_cfg_dict = sub_cfg.to_dict()
        # ç¡®ä¿æ¯ä¸ªå­é…ç½®ä¹Ÿæœ‰ model_type
        if "model_type" not in sub_cfg_dict:
            if hasattr(sub_cfg, "model_type"):
                sub_cfg_dict["model_type"] = sub_cfg.model_type
            elif "qwen2" in m.lower():
                sub_cfg_dict["model_type"] = "qwen2"
            elif "qwen3" in m.lower():
                sub_cfg_dict["model_type"] = "qwen3"
        sub_model_cfgs.append(sub_cfg_dict)
    
    config_dict.update({
        "num_sub_models": len(models),
        # "num_hidden_layers": config.num_hidden_layers * len(models),
        # "layer_types": config.layer_types * len(models),
        "architectures": ["QwenBoostForCausalLM"],   # è®©ä½ çš„ Ensemble ç±»è¢«è¯†åˆ«
        "is_llmboost": False,
        "sub_model_cfgs": sub_model_cfgs,
        "fusion_lambda": fusion_lambda,  # èåˆæƒé‡ï¼šå‰é¢çš„æ¨¡å‹å  (1-lambda)ï¼Œåé¢çš„æ¨¡å‹å  lambda
    })
    
    print(f"ğŸ”¹ ä¿å­˜çš„ model_type: {config_dict.get('model_type', 'æœªè®¾ç½®')}")

    with open(os.path.join(save_dir, "config.json"), "w") as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False)

    merged_state_dict = {}

    for idx, model in enumerate(models):
        sub_prefix = f"sub_models.{idx}"
        print(f"   â¤ Processing {sub_prefix} ...")

        # éå†æ¯ä¸ª weight
        for k, v in model.state_dict().items():
            merged_state_dict[f"{sub_prefix}.{k}"] = v.to(torch_dtype)

        # lm_head ï¼ˆæŸäº›æ¨¡å‹æ²¡æœ‰å•ç‹¬ lm_headï¼‰
        if hasattr(model, "lm_head") and hasattr(model.lm_head, "weight"):
            merged_state_dict[f"{sub_prefix}.lm_head.weight"] = model.lm_head.weight.to(torch_dtype)

    print(f"   âœ… Collected {len(merged_state_dict)} tensors")

    final_path = os.path.join(save_dir, "pytorch_model.bin")
    torch.save(merged_state_dict, final_path)

    print(f"\nğŸ‰ Fusion complete! Saved to: {final_path}")
    return final_path

def fuse_submodels_llmboost(model_list, save_dir, torch_dtype=torch.bfloat16):
    """
    å°†å¤šä¸ª CausalLM æ¨¡å‹åˆå¹¶ä¸ºä¸€ä¸ª Ensemble æ¨¡å‹çš„æƒé‡æ ¼å¼ã€‚

    Args:
        model_list (List[str]): æ¨¡å‹è·¯å¾„åˆ—è¡¨æˆ– HF åç§°åˆ—è¡¨
        save_dir (str): ä¿å­˜çš„ç›®å½•
        torch_dtype: ä¿å­˜çš„æƒé‡ç²¾åº¦ (é»˜è®¤ bfloat16)

    Returns:
        str: ä¿å­˜è·¯å¾„
    """

    print(f"ğŸ”¹ Loading {len(model_list)} models:")
    for m in model_list:
        print(f"   - {m}")

    os.makedirs(save_dir, exist_ok=True)

    print("\nğŸ”¹ Saving tokenizer ...")
    tokenizer = AutoTokenizer.from_pretrained(model_list[0])
    tokenizer.save_pretrained(save_dir)

    print("\nğŸ”¹ Loading model weights ...")
    models = [
        AutoModelForCausalLM.from_pretrained(m, torch_dtype=torch_dtype)
        for m in model_list
    ]

    print("\nğŸ”¹ Saving config.json ...")
    config = AutoConfig.from_pretrained(model_list[0])
    config_dict = config.to_dict()

    sub_model_cfgs = []
    for idx, m in enumerate(model_list):
        sub_cfg = AutoConfig.from_pretrained(m)
        sub_cfg_dict = sub_cfg.to_dict()
        sub_model_cfgs.append(sub_cfg_dict)
    config_dict.update({
        "num_sub_models": len(models),
        # "num_hidden_layers": config.num_hidden_layers * len(models),
        # "layer_types": config.layer_types * len(models),
        "architectures": ["QwenBoostForCausalLM"],   # è®©ä½ çš„ Ensemble ç±»è¢«è¯†åˆ«
        "is_llmboost": True,
        "sub_model_cfgs": sub_model_cfgs,
    })

    with open(os.path.join(save_dir, "config.json"), "w") as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False)

    merged_state_dict = {}

    for idx, model in enumerate(models):
        sub_prefix = f"sub_models.{idx}"
        print(f"   â¤ Processing {sub_prefix} ...")

        # éå†æ¯ä¸ª weight
        for k, v in model.state_dict().items():
            merged_state_dict[f"{sub_prefix}.{k}"] = v.to(torch_dtype)

        # lm_head ï¼ˆæŸäº›æ¨¡å‹æ²¡æœ‰å•ç‹¬ lm_headï¼‰
        if hasattr(model, "lm_head") and hasattr(model.lm_head, "weight"):
            merged_state_dict[f"{sub_prefix}.lm_head.weight"] = model.lm_head.weight.to(torch_dtype)

    print(f"   âœ… Collected {len(merged_state_dict)} tensors")

    final_path = os.path.join(save_dir, "pytorch_model.bin")
    torch.save(merged_state_dict, final_path)

    print(f"\nğŸ‰ Fusion complete! Saved to: {final_path}")
    return final_path

def _copy_prev_attn_qkv_as_pre(prev_model, merged_state_dict, tgt_sub_prefix: str, torch_dtype=torch.bfloat16):
    """
    æŠŠ prev_model çš„æ¯å±‚ attention q/k/v çº¿æ€§å±‚æƒé‡å¤åˆ¶åˆ°
    ç›®æ ‡å­æ¨¡å‹ (tgt_sub_prefix= 'sub_models.{idx}') çš„ prev_q/prev_k/prev_v é‡Œã€‚
    è¿™äº›æƒé‡ç”¨äºè®¡ç®— gateï¼šÏƒ(h^{(i-1)} W_Q^{(i-1)}) ç­‰ã€‚

    å…¼å®¹ä¸¤ç§æ ¼å¼ï¼š
      A) ...self_attn.q_proj/k_proj/v_proj.(weight|bias)
      B) ...self_attn.qkv_proj.(weight|bias)  (éœ€è¦ split æˆ q/k/v)
    """

    prev_sd = prev_model.state_dict()
    cfg = getattr(prev_model, "config", None)

    # ---------- A) åˆ†ç¦»å¼ q_proj/k_proj/v_proj ----------
    pat_sep = re.compile(r"^(.*self_attn)\.(q_proj|k_proj|v_proj)\.(weight|bias)$")
    found_sep = False
    copied_count = 0
    for pk, pv in prev_sd.items():
        m_sep = pat_sep.match(pk)
        if m_sep:
            found_sep = True
            attn_prefix, proj_type, wb = m_sep.group(1), m_sep.group(2), m_sep.group(3)
            # å°† q_proj/k_proj/v_proj æ˜ å°„åˆ° prev_q_proj/prev_k_proj/prev_v_proj
            target_k = f"{attn_prefix}.prev_{proj_type}.{wb}"
            full_key = f"{tgt_sub_prefix}.{target_k}"
            merged_state_dict[full_key] = pv.to(torch_dtype)
            copied_count += 1
    
    if found_sep:
        print(f"      âœ… Copied {copied_count} prev_*_proj weights from previous model to {tgt_sub_prefix}")
        return  # å¦‚æœå·²ç»æ‰¾åˆ°åˆ†ç¦»å¼ï¼Œå°±ä¸å†èµ°åˆå¹¶å¼

    # ---------- B) åˆå¹¶å¼ qkv_proj ----------
    pat_qkv = re.compile(r"^(.*self_attn)\.qkv_proj\.(weight|bias)$")
    # ç”¨ config æ¨ q/k/v çš„ out dimï¼›å¦‚æœ config ç¼ºå­—æ®µï¼Œå°±ä» tensor å½¢çŠ¶å°½é‡æ¨æ–­
    num_heads = getattr(cfg, "num_attention_heads", None)
    num_kv_heads = getattr(cfg, "num_key_value_heads", None) or getattr(cfg, "num_key_value_groups", None)
    hidden = getattr(cfg, "hidden_size", None)

    for k, v in prev_sd.items():
        m = pat_qkv.match(k)
        if not m:
            continue

        attn_prefix, wb = m.group(1), m.group(2)

        # v: [qkv_out, hidden] or bias: [qkv_out]
        qkv_out = v.shape[0]

        if num_heads is not None and hidden is not None:
            head_dim = hidden // num_heads
            if num_kv_heads is None:
                # fallbackï¼šå¸¸è§æ˜¯ MHA -> kv_heads=num_heads
                num_kv_heads = num_heads
            q_out = num_heads * head_dim
            kv_out = num_kv_heads * head_dim
            if q_out + 2 * kv_out != qkv_out:
                # å¦‚æœ config æ¨å‡ºæ¥ä¸ä¸€è‡´ï¼Œå°±ç”¨ shape å…œåº•ï¼šå‡è®¾ q:k:v = 1:1:1 ç­‰åˆ†ï¼ˆä¸ä¸€å®šå¯¹ï¼‰
                third = qkv_out // 3
                q_out, kv_out = third, third
        else:
            third = qkv_out // 3
            q_out, kv_out = third, third

        if wb == "weight":
            q_w, k_w, v_w = torch.split(v, [q_out, kv_out, kv_out], dim=0)
            merged_state_dict[f"{tgt_sub_prefix}.{attn_prefix}.prev_q_proj.weight"] = q_w.to(torch_dtype)
            merged_state_dict[f"{tgt_sub_prefix}.{attn_prefix}.prev_k_proj.weight"] = k_w.to(torch_dtype)
            merged_state_dict[f"{tgt_sub_prefix}.{attn_prefix}.prev_v_proj.weight"] = v_w.to(torch_dtype)
        else:
            q_b, k_b, v_b = torch.split(v, [q_out, kv_out, kv_out], dim=0)
            merged_state_dict[f"{tgt_sub_prefix}.{attn_prefix}.prev_q_proj.bias"] = q_b.to(torch_dtype)
            merged_state_dict[f"{tgt_sub_prefix}.{attn_prefix}.prev_k_proj.bias"] = k_b.to(torch_dtype)
            merged_state_dict[f"{tgt_sub_prefix}.{attn_prefix}.prev_v_proj.bias"] = v_b.to(torch_dtype)

def fuse_submodels_llmboost_gate(model_list, save_dir, torch_dtype=torch.bfloat16, gate_init=None, gate_position="g1"):
    """
    å°†å¤šä¸ª CausalLM æ¨¡å‹åˆå¹¶ä¸ºä¸€ä¸ª Ensemble æ¨¡å‹çš„æƒé‡æ ¼å¼ã€‚

    Args:
        model_list (List[str]): æ¨¡å‹è·¯å¾„åˆ—è¡¨æˆ– HF åç§°åˆ—è¡¨
        save_dir (str): ä¿å­˜çš„ç›®å½•
        torch_dtype: ä¿å­˜çš„æƒé‡ç²¾åº¦ (é»˜è®¤ bfloat16)

    Returns:
        str: ä¿å­˜è·¯å¾„
    """

    print(f"ğŸ”¹ Loading {len(model_list)} models:")
    for m in model_list:
        print(f"   - {m}")

    os.makedirs(save_dir, exist_ok=True)

    print("\nğŸ”¹ Saving tokenizer ...")
    tokenizer = AutoTokenizer.from_pretrained(model_list[0])
    tokenizer.save_pretrained(save_dir)

    print("\nğŸ”¹ Loading model weights ...")
    models = [
        AutoModelForCausalLM.from_pretrained(m, torch_dtype=torch_dtype)
        for m in model_list
    ]

    print("\nğŸ”¹ Saving config.json ...")
    
    # æ”¶é›†æ‰€æœ‰å­æ¨¡å‹çš„é…ç½®
    # æ³¨æ„ï¼šä¸èƒ½ç”¨ sub_configsï¼Œå› ä¸º transformers ä¼šæŠŠä»¥ _config ç»“å°¾çš„å±æ€§å½“ä½œå­é…ç½®å¤„ç†
    sub_model_cfgs = []
    for idx, m in enumerate(model_list):
        sub_cfg = AutoConfig.from_pretrained(m)
        sub_cfg_dict = sub_cfg.to_dict()
        # åç»­å­æ¨¡å‹å¯ç”¨ elementwise_attn_output_gate ä¸ prev-qkv gating
        if idx >= 1:
            sub_cfg_dict["elementwise_attn_output_gate"] = True
            sub_cfg_dict["use_prev_qkv_gate"] = True
        if gate_init is not None and idx >= 1:
            sub_cfg_dict["gate_init"] = gate_init
            sub_cfg_dict["gate_position"] = gate_position
        sub_model_cfgs.append(sub_cfg_dict)
    
    config = AutoConfig.from_pretrained(model_list[0])
    config_dict = config.to_dict()

    config_dict.update({
        "num_sub_models": len(models),
        # "num_hidden_layers": config.num_hidden_layers * len(models),
        # "layer_types": config.layer_types * len(models),
        "architectures": ["QwenBoostGateForCausalLM"],   # è®©ä½ çš„ Ensemble ç±»è¢«è¯†åˆ«
        "is_llmboost": True,
        "sub_model_cfgs": sub_model_cfgs,  # ä¿å­˜æ¯ä¸ªå­æ¨¡å‹çš„é…ç½®
    })

    with open(os.path.join(save_dir, "config.json"), "w") as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False)
    
    print(f"   âœ… Config saved (sub_model[1] elementwise_attn_output_gate=True)")

    merged_state_dict = {}

    for idx, model in enumerate(models):
        sub_prefix = f"sub_models.{idx}"
        print(f"   â¤ Processing {sub_prefix} ...")

        # éå†æ¯ä¸ª weight
        for k, v in model.state_dict().items():
            merged_state_dict[f"{sub_prefix}.{k}"] = v.to(torch_dtype)

        # lm_head ï¼ˆæŸäº›æ¨¡å‹æ²¡æœ‰å•ç‹¬ lm_headï¼‰
        if hasattr(model, "lm_head") and hasattr(model.lm_head, "weight"):
            merged_state_dict[f"{sub_prefix}.lm_head.weight"] = model.lm_head.weight.to(torch_dtype)

        # âœ… å…³é”®ï¼šæŠŠå‰ä¸€ä¸ªå­æ¨¡å‹çš„ attn qkv çº¿æ€§å±‚å¤åˆ¶åˆ°å½“å‰å­æ¨¡å‹çš„ prev_* é‡Œ
        # è¿™äº›æƒé‡ç”¨äºè®¡ç®— gateï¼šÏƒ(h^{(i-1)} W_Q^{(i-1)}) ç­‰
        if idx > 0:
            print(f"      â¤ Copying prev_*_proj weights from sub_models.{idx-1} to {sub_prefix}...")
            _copy_prev_attn_qkv_as_pre(
                prev_model=models[idx - 1],
                merged_state_dict=merged_state_dict,
                tgt_sub_prefix=sub_prefix,
                torch_dtype=torch_dtype,
            )

    # æ‰“å°å‡ºåˆå¹¶åçš„æ‰€æœ‰é”®
    print(f"\nğŸ”¹ All keys in merged_state_dict (total: {len(merged_state_dict)}):")
    all_keys = sorted(merged_state_dict.keys())
    for k in all_keys:
        shape = merged_state_dict[k].shape
        print(f"   - {k} | Shape: {shape}")
    
    # ç‰¹åˆ«æ£€æŸ¥ç¬¬äºŒä¸ªæ¨¡å‹çš„ prev_*_proj æƒé‡
    print(f"\nğŸ”¹ Checking prev_*_proj weights for sub_models.1 (second model):")
    sub_1_keys = [k for k in merged_state_dict.keys() if k.startswith("sub_models.1")]
    prev_proj_keys = [k for k in sub_1_keys if "prev_" in k]
    
    if prev_proj_keys:
        print(f"   âœ… Found {len(prev_proj_keys)} prev_*_proj weights:")
        for k in sorted(prev_proj_keys):
            print(f"      - {k} | Shape: {merged_state_dict[k].shape}")
    else:
        print(f"   âš ï¸  No prev_*_proj weights found in merged_state_dict!")
        print(f"   All sub_models.1 keys (showing first 30):")
        for k in sorted(sub_1_keys)[:30]:
            print(f"      - {k} | Shape: {merged_state_dict[k].shape}")
        if len(sub_1_keys) > 30:
            print(f"      ... and {len(sub_1_keys) - 30} more keys")

    final_path = os.path.join(save_dir, "pytorch_model.bin")
    torch.save(merged_state_dict, final_path)

    print(f"\nğŸ‰ Fusion complete! Saved to: {final_path}")
    return final_path

def fuse_submodels_llmboost_linear(model_list, save_dir, torch_dtype=torch.bfloat16):
    """
    å°†å¤šä¸ª CausalLM æ¨¡å‹åˆå¹¶ä¸ºä¸€ä¸ª Ensemble æ¨¡å‹çš„æƒé‡æ ¼å¼ï¼ˆLinear åŠ æ³•èåˆæ–¹å¼ï¼‰ã€‚
    
    å…¬å¼ï¼šQ^{(i)} = h^{(i)} W_Q^{(i)} + h^{(i-1)} W_Q^{(i-1)}
    
    Args:
        model_list (List[str]): æ¨¡å‹è·¯å¾„åˆ—è¡¨æˆ– HF åç§°åˆ—è¡¨
        save_dir (str): ä¿å­˜çš„ç›®å½•
        torch_dtype: ä¿å­˜çš„æƒé‡ç²¾åº¦ (é»˜è®¤ bfloat16)

    Returns:
        str: ä¿å­˜è·¯å¾„
    """

    print(f"ğŸ”¹ Loading {len(model_list)} models:")
    for m in model_list:
        print(f"   - {m}")

    os.makedirs(save_dir, exist_ok=True)

    print("\nğŸ”¹ Saving tokenizer ...")
    tokenizer = AutoTokenizer.from_pretrained(model_list[0])
    tokenizer.save_pretrained(save_dir)

    print("\nğŸ”¹ Loading model weights ...")
    models = [
        AutoModelForCausalLM.from_pretrained(m, torch_dtype=torch_dtype)
        for m in model_list
    ]

    print("\nğŸ”¹ Saving config.json ...")
    
    # æ”¶é›†æ‰€æœ‰å­æ¨¡å‹çš„é…ç½®
    sub_model_cfgs = []
    for idx, m in enumerate(model_list):
        sub_cfg = AutoConfig.from_pretrained(m)
        sub_cfg_dict = sub_cfg.to_dict()
        # åç»­å­æ¨¡å‹å¯ç”¨ use_prev_qkv_linearï¼ˆçº¿æ€§åŠ æ³•èåˆï¼‰
        if idx >= 1:
            sub_cfg_dict["use_prev_qkv_linear"] = True
        sub_model_cfgs.append(sub_cfg_dict)
    
    config = AutoConfig.from_pretrained(model_list[0])
    config_dict = config.to_dict()

    config_dict.update({
        "num_sub_models": len(models),
        # "num_hidden_layers": config.num_hidden_layers * len(models),
        # "layer_types": config.layer_types * len(models),
        "architectures": ["QwenBoostGateForCausalLM"],   # ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ç±»
        "is_llmboost": True,
        "sub_model_cfgs": sub_model_cfgs,
    })

    with open(os.path.join(save_dir, "config.json"), "w") as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False)
    
    print(f"   âœ… Config saved (sub_model[1] use_prev_qkv_linear=True)")

    merged_state_dict = {}

    for idx, model in enumerate(models):
        sub_prefix = f"sub_models.{idx}"
        print(f"   â¤ Processing {sub_prefix} ...")

        # éå†æ¯ä¸ª weight
        for k, v in model.state_dict().items():
            merged_state_dict[f"{sub_prefix}.{k}"] = v.to(torch_dtype)

        # lm_head ï¼ˆæŸäº›æ¨¡å‹æ²¡æœ‰å•ç‹¬ lm_headï¼‰
        if hasattr(model, "lm_head") and hasattr(model.lm_head, "weight"):
            merged_state_dict[f"{sub_prefix}.lm_head.weight"] = model.lm_head.weight.to(torch_dtype)

        # âœ… å…³é”®ï¼šæŠŠå‰ä¸€ä¸ªå­æ¨¡å‹çš„ attn qkv çº¿æ€§å±‚å¤åˆ¶åˆ°å½“å‰å­æ¨¡å‹çš„ prev_* é‡Œ
        # è¿™äº›æƒé‡ç”¨äº linear èåˆï¼šh^{(i-1)} W_Q^{(i-1)} ç­‰
        if idx > 0:
            print(f"      â¤ Copying prev_*_proj weights from sub_models.{idx-1} to {sub_prefix}...")
            _copy_prev_attn_qkv_as_pre(
                prev_model=models[idx - 1],
                merged_state_dict=merged_state_dict,
                tgt_sub_prefix=sub_prefix,
                torch_dtype=torch_dtype,
            )

    # æ‰“å°å‡ºåˆå¹¶åçš„æ‰€æœ‰é”®
    print(f"\nğŸ”¹ All keys in merged_state_dict (total: {len(merged_state_dict)}):")
    all_keys = sorted(merged_state_dict.keys())
    for k in all_keys:
        shape = merged_state_dict[k].shape
        print(f"   - {k} | Shape: {shape}")
    
    # ç‰¹åˆ«æ£€æŸ¥ç¬¬äºŒä¸ªæ¨¡å‹çš„ prev_*_proj æƒé‡
    print(f"\nğŸ”¹ Checking prev_*_proj weights for sub_models.1 (second model):")
    sub_1_keys = [k for k in merged_state_dict.keys() if k.startswith("sub_models.1")]
    prev_proj_keys = [k for k in sub_1_keys if "prev_" in k]
    
    if prev_proj_keys:
        print(f"   âœ… Found {len(prev_proj_keys)} prev_*_proj weights:")
        for k in sorted(prev_proj_keys):
            print(f"      - {k} | Shape: {merged_state_dict[k].shape}")
    else:
        print(f"   âš ï¸  No prev_*_proj weights found in merged_state_dict!")
        print(f"   All sub_models.1 keys (showing first 30):")
        for k in sorted(sub_1_keys)[:30]:
            print(f"      - {k} | Shape: {merged_state_dict[k].shape}")
        if len(sub_1_keys) > 30:
            print(f"      ... and {len(sub_1_keys) - 30} more keys")

    final_path = os.path.join(save_dir, "pytorch_model.bin")
    torch.save(merged_state_dict, final_path)

    print(f"\nğŸ‰ Fusion complete! Saved to: {final_path}")
    return final_path

def fuse_submodels_llmboost_cross_model_attention(model_list, save_dir, torch_dtype=torch.bfloat16):
    """
    å°†å¤šä¸ª CausalLM æ¨¡å‹åˆå¹¶ä¸ºä¸€ä¸ª Ensemble æ¨¡å‹çš„æƒé‡æ ¼å¼ï¼ˆcross model attention èåˆæ–¹å¼ï¼‰ã€‚
    
    å…¬å¼ï¼šQ^{(i)} = h^{(i)} W_Q^{(i)} + h^{(i-1)} W_Q^{(i-1)}
    
    Args:
        model_list (List[str]): æ¨¡å‹è·¯å¾„åˆ—è¡¨æˆ– HF åç§°åˆ—è¡¨
        save_dir (str): ä¿å­˜çš„ç›®å½•
        torch_dtype: ä¿å­˜çš„æƒé‡ç²¾åº¦ (é»˜è®¤ bfloat16)

    Returns:
        str: ä¿å­˜è·¯å¾„
    """

    print(f"ğŸ”¹ Loading {len(model_list)} models:")
    for m in model_list:
        print(f"   - {m}")

    os.makedirs(save_dir, exist_ok=True)

    print("\nğŸ”¹ Saving tokenizer ...")
    tokenizer = AutoTokenizer.from_pretrained(model_list[0])
    tokenizer.save_pretrained(save_dir)

    print("\nğŸ”¹ Loading model weights ...")
    models = [
        AutoModelForCausalLM.from_pretrained(m, torch_dtype=torch_dtype)
        for m in model_list
    ]

    print("\nğŸ”¹ Saving config.json ...")
    
    # æ”¶é›†æ‰€æœ‰å­æ¨¡å‹çš„é…ç½®
    sub_model_cfgs = []
    for idx, m in enumerate(model_list):
        sub_cfg = AutoConfig.from_pretrained(m)
        sub_cfg_dict = sub_cfg.to_dict()
        # åç»­å­æ¨¡å‹å¯ç”¨ use_cross_model_attention
        if idx >= 1:
            sub_cfg_dict["use_cross_model_attention"] = True
        sub_model_cfgs.append(sub_cfg_dict)
    
    config = AutoConfig.from_pretrained(model_list[0])
    config_dict = config.to_dict()

    config_dict.update({
        "num_sub_models": len(models),
        # "num_hidden_layers": config.num_hidden_layers * len(models),
        # "layer_types": config.layer_types * len(models),
        "architectures": ["QwenBoostGateForCausalLM"],   # ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ç±»
        "is_llmboost": True,
        "sub_model_cfgs": sub_model_cfgs,
    })

    with open(os.path.join(save_dir, "config.json"), "w") as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False)
    
    print(f"   âœ… Config saved (sub_model[1] use_cross_model_attention=True)")

    merged_state_dict = {}

    for idx, model in enumerate(models):
        sub_prefix = f"sub_models.{idx}"
        print(f"   â¤ Processing {sub_prefix} ...")

        # éå†æ¯ä¸ª weight
        for k, v in model.state_dict().items():
            merged_state_dict[f"{sub_prefix}.{k}"] = v.to(torch_dtype)

        # lm_head ï¼ˆæŸäº›æ¨¡å‹æ²¡æœ‰å•ç‹¬ lm_headï¼‰
        if hasattr(model, "lm_head") and hasattr(model.lm_head, "weight"):
            merged_state_dict[f"{sub_prefix}.lm_head.weight"] = model.lm_head.weight.to(torch_dtype)

        # âœ… å…³é”®ï¼šæŠŠå‰ä¸€ä¸ªå­æ¨¡å‹çš„ attn qkv çº¿æ€§å±‚å¤åˆ¶åˆ°å½“å‰å­æ¨¡å‹çš„ prev_* é‡Œ
        # è¿™äº›æƒé‡ç”¨äº linear èåˆï¼šh^{(i-1)} W_Q^{(i-1)} ç­‰
        # if idx > 0:
        #     print(f"      â¤ Copying prev_*_proj weights from sub_models.{idx-1} to {sub_prefix}...")
        #     _copy_prev_attn_qkv_as_pre(
        #         prev_model=models[idx - 1],
        #         merged_state_dict=merged_state_dict,
        #         tgt_sub_prefix=sub_prefix,
        #         torch_dtype=torch_dtype,
        #     )

    # æ‰“å°å‡ºåˆå¹¶åçš„æ‰€æœ‰é”®
    # print(f"\nğŸ”¹ All keys in merged_state_dict (total: {len(merged_state_dict)}):")
    # all_keys = sorted(merged_state_dict.keys())
    # for k in all_keys:
    #     shape = merged_state_dict[k].shape
    #     print(f"   - {k} | Shape: {shape}")
    
    # # ç‰¹åˆ«æ£€æŸ¥ç¬¬äºŒä¸ªæ¨¡å‹çš„ prev_*_proj æƒé‡
    # print(f"\nğŸ”¹ Checking prev_*_proj weights for sub_models.1 (second model):")
    # sub_1_keys = [k for k in merged_state_dict.keys() if k.startswith("sub_models.1")]
    # prev_proj_keys = [k for k in sub_1_keys if "prev_" in k]
    
    # if prev_proj_keys:
    #     print(f"   âœ… Found {len(prev_proj_keys)} prev_*_proj weights:")
    #     for k in sorted(prev_proj_keys):
    #         print(f"      - {k} | Shape: {merged_state_dict[k].shape}")
    # else:
    #     print(f"   âš ï¸  No prev_*_proj weights found in merged_state_dict!")
    #     print(f"   All sub_models.1 keys (showing first 30):")
    #     for k in sorted(sub_1_keys)[:30]:
    #         print(f"      - {k} | Shape: {merged_state_dict[k].shape}")
    #     if len(sub_1_keys) > 30:
    #         print(f"      ... and {len(sub_1_keys) - 30} more keys")

    final_path = os.path.join(save_dir, "pytorch_model.bin")
    torch.save(merged_state_dict, final_path)

    print(f"\nğŸ‰ Fusion complete! Saved to: {final_path}")
    return final_path

def fuse_submodels_llmboost_sharekv(model_list, save_dir, torch_dtype=torch.bfloat16):
    """
    å°†å¤šä¸ª CausalLM æ¨¡å‹åˆå¹¶ä¸ºä¸€ä¸ª Ensemble æ¨¡å‹çš„æƒé‡æ ¼å¼ï¼ˆcross model attention èåˆæ–¹å¼ï¼‰ã€‚
    
    å…¬å¼ï¼šQ^{(i)} = h^{(i)} W_Q^{(i)} + h^{(i-1)} W_Q^{(i-1)}
    
    Args:
        model_list (List[str]): æ¨¡å‹è·¯å¾„åˆ—è¡¨æˆ– HF åç§°åˆ—è¡¨
        save_dir (str): ä¿å­˜çš„ç›®å½•
        torch_dtype: ä¿å­˜çš„æƒé‡ç²¾åº¦ (é»˜è®¤ bfloat16)

    Returns:
        str: ä¿å­˜è·¯å¾„
    """

    print(f"ğŸ”¹ Loading {len(model_list)} models:")
    for m in model_list:
        print(f"   - {m}")

    os.makedirs(save_dir, exist_ok=True)

    print("\nğŸ”¹ Saving tokenizer ...")
    tokenizer = AutoTokenizer.from_pretrained(model_list[0])
    tokenizer.save_pretrained(save_dir)

    print("\nğŸ”¹ Loading model weights ...")
    models = [
        AutoModelForCausalLM.from_pretrained(m, torch_dtype=torch_dtype)
        for m in model_list
    ]

    print("\nğŸ”¹ Saving config.json ...")
    
    # æ”¶é›†æ‰€æœ‰å­æ¨¡å‹çš„é…ç½®
    sub_model_cfgs = []
    for idx, m in enumerate(model_list):
        sub_cfg = AutoConfig.from_pretrained(m)
        sub_cfg_dict = sub_cfg.to_dict()
        sub_model_cfgs.append(sub_cfg_dict)
    
    config = AutoConfig.from_pretrained(model_list[0])
    config_dict = config.to_dict()

    config_dict.update({
        "num_sub_models": len(models),
        # "num_hidden_layers": config.num_hidden_layers * len(models),
        # "layer_types": config.layer_types * len(models),
        "architectures": ["QwenBoostGateForCausalLM"],   # ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ç±»
        "is_llmboost": True,
        "sub_model_cfgs": sub_model_cfgs,
        "use_share_kv": True, # å¯ç”¨ share kv èåˆæ–¹å¼
    })

    with open(os.path.join(save_dir, "config.json"), "w") as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False)
    
    print(f"   âœ… Config saved (sub_model[1] use_cross_model_attention=True)")

    merged_state_dict = {}

    for idx, model in enumerate(models):
        sub_prefix = f"sub_models.{idx}"
        print(f"   â¤ Processing {sub_prefix} ...")

        # éå†æ¯ä¸ª weight
        for k, v in model.state_dict().items():
            merged_state_dict[f"{sub_prefix}.{k}"] = v.to(torch_dtype)

        # lm_head ï¼ˆæŸäº›æ¨¡å‹æ²¡æœ‰å•ç‹¬ lm_headï¼‰
        if hasattr(model, "lm_head") and hasattr(model.lm_head, "weight"):
            merged_state_dict[f"{sub_prefix}.lm_head.weight"] = model.lm_head.weight.to(torch_dtype)

    final_path = os.path.join(save_dir, "pytorch_model.bin")
    torch.save(merged_state_dict, final_path)

    print(f"\nğŸ‰ Fusion complete! Saved to: {final_path}")
    return final_path

def load_fuse_model_tokenizer(model_name):
    model = QwenBoostForCausalLM.from_pretrained(
        model_name,
        torch_dtype="auto",
        device_map=None,      # â—â—ä¸è¦è‡ªåŠ¨æ”¾ GPU
        # low_cpu_mem_usage=True,
        attn_implementation="flash_attention_2"
    )

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    return model, tokenizer

def load_fuse_model_tokenizer_vote(model_name, freeze_first_model):
    # åˆ¤æ–­æ˜¯ qwen2 è¿˜æ˜¯ qwen3ï¼šå…ˆå°è¯•åŠ è½½ config æ£€æŸ¥
    is_qwen2 = False
    try:
        config = AutoConfig.from_pretrained(model_name)
        # ä¼˜å…ˆæ£€æŸ¥ config ä¸­çš„ model_type
        if hasattr(config, 'model_type') and config.model_type:
            model_type_str = str(config.model_type).lower()
            is_qwen2 = 'qwen2' in model_type_str
            print(f"ğŸ”¹ ä» config.model_type è¯†åˆ«: {config.model_type} -> {'Qwen2' if is_qwen2 else 'Qwen3'}")
        # å¦‚æœ model_type æ— æ³•ç¡®å®šï¼Œæ£€æŸ¥ hidden_sizeï¼ˆqwen2.5-3b é€šå¸¸æ˜¯ 2048ï¼‰
        if not is_qwen2 and hasattr(config, 'hidden_size'):
            hidden_size = config.hidden_size
            # qwen2.5-3b çš„ hidden_size æ˜¯ 2048ï¼Œqwen3 é€šå¸¸æ˜¯ 2560 æˆ–æ›´å¤§
            if hidden_size == 2048:
                is_qwen2 = True
                print(f"ğŸ”¹ ä» hidden_size={hidden_size} æ¨æ–­ä¸º Qwen2")
            elif hidden_size >= 2560:
                is_qwen2 = False
                print(f"ğŸ”¹ ä» hidden_size={hidden_size} æ¨æ–­ä¸º Qwen3")
        # å¦‚æœè¿˜æ˜¯æ— æ³•ç¡®å®šï¼Œæ£€æŸ¥ architectures
        if not is_qwen2 and hasattr(config, 'architectures') and config.architectures:
            is_qwen2 = any('qwen2' in str(arch).lower() for arch in config.architectures)
            if is_qwen2:
                print(f"ğŸ”¹ ä» architectures è¯†åˆ«ä¸º Qwen2")
        # æœ€åæ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not is_qwen2:
            is_qwen2 = 'qwen2' in model_name.lower()
            if is_qwen2:
                print(f"ğŸ”¹ ä»æ¨¡å‹è·¯å¾„è¯†åˆ«ä¸º Qwen2")
    except Exception as e:
        # å¦‚æœåŠ è½½ config å¤±è´¥ï¼Œæ ¹æ®è·¯å¾„åˆ¤æ–­
        print(f"âš ï¸  åŠ è½½ config å¤±è´¥: {e}ï¼Œä½¿ç”¨è·¯å¾„åˆ¤æ–­")
        is_qwen2 = 'qwen2' in model_name.lower()
    
    # æ ¹æ®åˆ¤æ–­ç»“æœå¯¼å…¥ç›¸åº”çš„æ¨¡å—
    if is_qwen2:
        try:
            from EnsembleLLM.EnsembleQwen2LLMBOOST.modeling_qwen2 import QwenBoostForCausalLM
        except ImportError:
            try:
                from EnsembleQwen2LLMBOOST.modeling_qwen2 import QwenBoostForCausalLM
            except ImportError as exc:
                _current_dir = os.path.dirname(os.path.abspath(__file__))
                _project_root = os.path.abspath(os.path.join(_current_dir, ".."))
                if _project_root not in sys.path:
                    sys.path.insert(0, _project_root)
                try:
                    from EnsembleQwen2LLMBOOST.modeling_qwen2 import QwenBoostForCausalLM
                except ImportError:
                    raise ImportError("Cannot locate `Qwen2ForEnsemble`. Please check PYTHONPATH.") from exc
        print("ğŸ”¹ ä½¿ç”¨ Qwen2 æ¨¡å‹ (vote-base)")
    else:
        try:
            from EnsembleLLM.EnsembleQwen3.modeling_qwen3 import QwenBoostForCausalLM
        except ImportError:
            try:
                from EnsembleQwen3.modeling_qwen3 import QwenBoostForCausalLM
            except ImportError as exc:
                _current_dir = os.path.dirname(os.path.abspath(__file__))
                _project_root = os.path.abspath(os.path.join(_current_dir, ".."))
                if _project_root not in sys.path:
                    sys.path.insert(0, _project_root)
                try:
                    from EnsembleQwen3.modeling_qwen3 import QwenBoostForCausalLM
                except ImportError:
                    raise ImportError("Cannot locate `Qwen3ForEnsemble`. Please check PYTHONPATH.") from exc
        print("ğŸ”¹ ä½¿ç”¨ Qwen3 æ¨¡å‹ (vote-base)")
    
    model = QwenBoostForCausalLM.from_pretrained(
        model_name,
        torch_dtype="auto",
        device_map=None,      # â—â—ä¸è¦è‡ªåŠ¨æ”¾ GPU
        # low_cpu_mem_usage=True,
        attn_implementation="flash_attention_2"
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    # å¦‚æœéœ€è¦å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼ˆsub_models.0ï¼‰ï¼Œåˆ™å°†å…¶æ‰€æœ‰å‚æ•°çš„ requires_grad è®¾ç½®ä¸º False
    # å¤„ç† freeze_first_model å‚æ•°ï¼Œæ”¯æŒ bool ç±»å‹å’Œå­—ç¬¦ä¸²ç±»å‹
    should_freeze = False
    if isinstance(freeze_first_model, bool):
        should_freeze = freeze_first_model
    elif isinstance(freeze_first_model, str):
        should_freeze = freeze_first_model.lower() in ("true", "1", "yes", "on")
    else:
        should_freeze = bool(freeze_first_model)
    
    if should_freeze:
        frozen_count = 0
        for name, param in model.named_parameters():
            if name.startswith("sub_models.0"):
                param.requires_grad = False
                frozen_count += 1
        print(f"âœ… å·²å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ (sub_models.0) çš„ {frozen_count} ä¸ªå‚æ•°ï¼Œè¿™äº›å‚æ•°å°†ä¸å‚ä¸è®­ç»ƒ")
    else:
        print("â„¹ï¸  æœªå¯ç”¨å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œæ‰€æœ‰å‚æ•°éƒ½å°†å‚ä¸è®­ç»ƒ")
    return model, tokenizer

def load_fuse_model_tokenizer_llmboost(model_name, freeze_first_model ):
    try:
        from EnsembleLLM.EnsembleQwen3LLMBOOST.modeling_qwen3 import QwenBoostForCausalLM
    except ImportError:
        try:
            from EnsembleQwen3LLMBOOST.modeling_qwen3 import QwenBoostForCausalLM
        except ImportError as exc:
            _current_dir = os.path.dirname(os.path.abspath(__file__))
            _project_root = os.path.abspath(os.path.join(_current_dir, ".."))
            if _project_root not in sys.path:
                sys.path.insert(0, _project_root)
            try:
                from EnsembleQwen3LLMBOOST.modeling_qwen3 import QwenBoostForCausalLM
            except ImportError:
                raise ImportError("Cannot locate `Qwen3ForEnsemble`. Please check PYTHONPATH.") from exc
    model = QwenBoostForCausalLM.from_pretrained(
        model_name,
        torch_dtype="auto",
        device_map=None,      # â—â—ä¸è¦è‡ªåŠ¨æ”¾ GPU
        # low_cpu_mem_usage=True,
        attn_implementation="flash_attention_2"
    )
    
    # å¦‚æœéœ€è¦å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼ˆsub_models.0ï¼‰ï¼Œåˆ™å°†å…¶æ‰€æœ‰å‚æ•°çš„ requires_grad è®¾ç½®ä¸º False
    # å¤„ç† freeze_first_model å‚æ•°ï¼Œæ”¯æŒ bool ç±»å‹å’Œå­—ç¬¦ä¸²ç±»å‹
    should_freeze = False
    if isinstance(freeze_first_model, bool):
        should_freeze = freeze_first_model
    elif isinstance(freeze_first_model, str):
        should_freeze = freeze_first_model.lower() in ("true", "1", "yes", "on")
    else:
        should_freeze = bool(freeze_first_model)
    
    if should_freeze:
        frozen_count = 0
        for name, param in model.named_parameters():
            if name.startswith("sub_models.0"):
                param.requires_grad = False
                frozen_count += 1
        print(f"âœ… å·²å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ (sub_models.0) çš„ {frozen_count} ä¸ªå‚æ•°ï¼Œè¿™äº›å‚æ•°å°†ä¸å‚ä¸è®­ç»ƒ")
    else:
        print("â„¹ï¸  æœªå¯ç”¨å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œæ‰€æœ‰å‚æ•°éƒ½å°†å‚ä¸è®­ç»ƒ")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    return model, tokenizer

def load_fuse_model_tokenizer_llmboost_qwen2(model_name, freeze_first_model ):
    try:
        from EnsembleLLM.EnsembleQwen2LLMBOOST.modeling_qwen2 import QwenBoostForCausalLM
    except ImportError:
        try:
            from EnsembleQwen2LLMBOOST.modeling_qwen2 import QwenBoostForCausalLM
        except ImportError as exc:
            _current_dir = os.path.dirname(os.path.abspath(__file__))
            _project_root = os.path.abspath(os.path.join(_current_dir, ".."))
            if _project_root not in sys.path:
                sys.path.insert(0, _project_root)
            try:
                from EnsembleQwen2LLMBOOST.modeling_qwen2 import QwenBoostForCausalLM
            except ImportError:
                raise ImportError("Cannot locate `Qwen2ForEnsemble`. Please check PYTHONPATH.") from exc
    model = QwenBoostForCausalLM.from_pretrained(
        model_name,
        torch_dtype="auto",
        device_map=None,      # â—â—ä¸è¦è‡ªåŠ¨æ”¾ GPU
        # low_cpu_mem_usage=True,
        attn_implementation="flash_attention_2"
    )
    
    # å¦‚æœéœ€è¦å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼ˆsub_models.0ï¼‰ï¼Œåˆ™å°†å…¶æ‰€æœ‰å‚æ•°çš„ requires_grad è®¾ç½®ä¸º False
    # å¤„ç† freeze_first_model å‚æ•°ï¼Œæ”¯æŒ bool ç±»å‹å’Œå­—ç¬¦ä¸²ç±»å‹
    should_freeze = False
    if isinstance(freeze_first_model, bool):
        should_freeze = freeze_first_model
    elif isinstance(freeze_first_model, str):
        should_freeze = freeze_first_model.lower() in ("true", "1", "yes", "on")
    else:
        should_freeze = bool(freeze_first_model)
    
    if should_freeze:
        frozen_count = 0
        for name, param in model.named_parameters():
            if name.startswith("sub_models.0"):
                param.requires_grad = False
                frozen_count += 1
        print(f"âœ… å·²å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ (sub_models.0) çš„ {frozen_count} ä¸ªå‚æ•°ï¼Œè¿™äº›å‚æ•°å°†ä¸å‚ä¸è®­ç»ƒ")
    else:
        print("â„¹ï¸  æœªå¯ç”¨å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œæ‰€æœ‰å‚æ•°éƒ½å°†å‚ä¸è®­ç»ƒ")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    return model, tokenizer

def load_fuse_model_tokenizer_llmboost_gate(model_name, freeze_first_model):
    try:
        from EnsembleLLM.EnsembleQwen3LLMBOOSTGATE.modeling_qwen3 import QwenBoostGateForCausalLM
        from EnsembleLLM.EnsembleQwen3LLMBOOSTGATE.configuration_qwen3 import Qwen3Config
    except ImportError:
        try:
            from EnsembleQwen3LLMBOOSTGATE.modeling_qwen3 import QwenBoostGateForCausalLM
            from EnsembleQwen3LLMBOOSTGATE.configuration_qwen3 import Qwen3Config
        except ImportError as exc:
            _current_dir = os.path.dirname(os.path.abspath(__file__))
            _project_root = os.path.abspath(os.path.join(_current_dir, ".."))
            if _project_root not in sys.path:
                sys.path.insert(0, _project_root)
            try:
                from EnsembleQwen3LLMBOOSTGATE.modeling_qwen3 import QwenBoostGateForCausalLM
                from EnsembleQwen3LLMBOOSTGATE.configuration_qwen3 import Qwen3Config
            except ImportError:
                raise ImportError("Cannot locate `Qwen3ForEnsemble`. Please check PYTHONPATH.") from exc
    
    # ä½¿ç”¨ from_pretrained åŠ è½½ï¼Œå®ƒå†…éƒ¨å·²ç»æ˜¯ strict=False
    # ç¼ºå¤±çš„æƒé‡ï¼ˆå¦‚ pre_projï¼‰ä¼šä¿æŒéšæœºåˆå§‹åŒ–
    model = QwenBoostGateForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map=None,
        attn_implementation="flash_attention_2",
        low_cpu_mem_usage=True,  # å‡å°‘å†…å­˜ä½¿ç”¨
    )
    print(f"âœ… Model loaded with dtype: {next(model.parameters()).dtype}")
    # å¦‚æœéœ€è¦å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼ˆsub_models.0ï¼‰ï¼Œåˆ™å°†å…¶æ‰€æœ‰å‚æ•°çš„ requires_grad è®¾ç½®ä¸º False
    # å¤„ç† freeze_first_model å‚æ•°ï¼Œæ”¯æŒ bool ç±»å‹å’Œå­—ç¬¦ä¸²ç±»å‹
    should_freeze = False
    if isinstance(freeze_first_model, bool):
        should_freeze = freeze_first_model
    elif isinstance(freeze_first_model, str):
        should_freeze = freeze_first_model.lower() in ("true", "1", "yes", "on")
    else:
        should_freeze = bool(freeze_first_model)
    
    if should_freeze:
        frozen_count = 0
        for name, param in model.named_parameters():
            if name.startswith("sub_models.0"):
                param.requires_grad = False
                frozen_count += 1
        print(f"âœ… å·²å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ (sub_models.0) çš„ {frozen_count} ä¸ªå‚æ•°ï¼Œè¿™äº›å‚æ•°å°†ä¸å‚ä¸è®­ç»ƒ")
    else:
        print("â„¹ï¸  æœªå¯ç”¨å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œæ‰€æœ‰å‚æ•°éƒ½å°†å‚ä¸è®­ç»ƒ")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    return model, tokenizer

def load_fuse_model_tokenizer_llmboost_linear(model_name, freeze_first_model):
    """åŠ è½½ llmboost-linear èåˆæ¨¡å‹ï¼ˆä½¿ç”¨åŠ æ³•èåˆï¼‰"""
    try:
        from EnsembleLLM.EnsembleQwen3LLMBOOSTGATE.modeling_qwen3 import QwenBoostGateForCausalLM
        from EnsembleLLM.EnsembleQwen3LLMBOOSTGATE.configuration_qwen3 import Qwen3Config
    except ImportError:
        try:
            from EnsembleQwen3LLMBOOSTGATE.modeling_qwen3 import QwenBoostGateForCausalLM
            from EnsembleQwen3LLMBOOSTGATE.configuration_qwen3 import Qwen3Config
        except ImportError as exc:
            _current_dir = os.path.dirname(os.path.abspath(__file__))
            _project_root = os.path.abspath(os.path.join(_current_dir, ".."))
            if _project_root not in sys.path:
                sys.path.insert(0, _project_root)
            try:
                from EnsembleQwen3LLMBOOSTGATE.modeling_qwen3 import QwenBoostGateForCausalLM
                from EnsembleQwen3LLMBOOSTGATE.configuration_qwen3 import Qwen3Config
            except ImportError:
                raise ImportError("Cannot locate `QwenBoostGateForCausalLM`. Please check PYTHONPATH.") from exc
    
    # ä½¿ç”¨ from_pretrained åŠ è½½
    model = QwenBoostGateForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map=None,
        attn_implementation="flash_attention_2",
        low_cpu_mem_usage=True,
    )
    print(f"âœ… Model (llmboost-linear) loaded with dtype: {next(model.parameters()).dtype}")
    # å¦‚æœéœ€è¦å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼ˆsub_models.0ï¼‰ï¼Œåˆ™å°†å…¶æ‰€æœ‰å‚æ•°çš„ requires_grad è®¾ç½®ä¸º False
    # å¤„ç† freeze_first_model å‚æ•°ï¼Œæ”¯æŒ bool ç±»å‹å’Œå­—ç¬¦ä¸²ç±»å‹
    should_freeze = False
    if isinstance(freeze_first_model, bool):
        should_freeze = freeze_first_model
    elif isinstance(freeze_first_model, str):
        should_freeze = freeze_first_model.lower() in ("true", "1", "yes", "on")
    else:
        should_freeze = bool(freeze_first_model)
    
    if should_freeze:
        frozen_count = 0
        for name, param in model.named_parameters():
            if name.startswith("sub_models.0"):
                param.requires_grad = False
                frozen_count += 1
        print(f"âœ… å·²å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ (sub_models.0) çš„ {frozen_count} ä¸ªå‚æ•°ï¼Œè¿™äº›å‚æ•°å°†ä¸å‚ä¸è®­ç»ƒ")
    else:
        print("â„¹ï¸  æœªå¯ç”¨å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œæ‰€æœ‰å‚æ•°éƒ½å°†å‚ä¸è®­ç»ƒ")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    return model, tokenizer

def load_fuse_model_tokenizer_llmboost_cross_model_attention(model_name, freeze_first_model):
    """åŠ è½½ llmboost-linear èåˆæ¨¡å‹ï¼ˆä½¿ç”¨åŠ æ³•èåˆï¼‰"""
    try:
        from EnsembleLLM.EnsembleQwen3LLMBOOSTGATE.modeling_qwen3 import QwenBoostGateForCausalLM
        from EnsembleLLM.EnsembleQwen3LLMBOOSTGATE.configuration_qwen3 import Qwen3Config
    except ImportError:
        try:
            from EnsembleQwen3LLMBOOSTGATE.modeling_qwen3 import QwenBoostGateForCausalLM
            from EnsembleQwen3LLMBOOSTGATE.configuration_qwen3 import Qwen3Config
        except ImportError as exc:
            _current_dir = os.path.dirname(os.path.abspath(__file__))
            _project_root = os.path.abspath(os.path.join(_current_dir, ".."))
            if _project_root not in sys.path:
                sys.path.insert(0, _project_root)
            try:
                from EnsembleQwen3LLMBOOSTGATE.modeling_qwen3 import QwenBoostGateForCausalLM
                from EnsembleQwen3LLMBOOSTGATE.configuration_qwen3 import Qwen3Config
            except ImportError:
                raise ImportError("Cannot locate `QwenBoostGateForCausalLM`. Please check PYTHONPATH.") from exc
    
    # ä½¿ç”¨ from_pretrained åŠ è½½
    model = QwenBoostGateForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map=None,
        attn_implementation="flash_attention_2",
        low_cpu_mem_usage=True,
    )
    print(f"âœ… Model (llmboost-linear) loaded with dtype: {next(model.parameters()).dtype}")
    # å¦‚æœéœ€è¦å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼ˆsub_models.0ï¼‰ï¼Œåˆ™å°†å…¶æ‰€æœ‰å‚æ•°çš„ requires_grad è®¾ç½®ä¸º False
    # å¤„ç† freeze_first_model å‚æ•°ï¼Œæ”¯æŒ bool ç±»å‹å’Œå­—ç¬¦ä¸²ç±»å‹
    should_freeze = False
    if isinstance(freeze_first_model, bool):
        should_freeze = freeze_first_model
    elif isinstance(freeze_first_model, str):
        should_freeze = freeze_first_model.lower() in ("true", "1", "yes", "on")
    else:
        should_freeze = bool(freeze_first_model)
    
    if should_freeze:
        frozen_count = 0
        for name, param in model.named_parameters():
            if name.startswith("sub_models.0"):
                param.requires_grad = False
                frozen_count += 1
        print(f"âœ… å·²å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ (sub_models.0) çš„ {frozen_count} ä¸ªå‚æ•°ï¼Œè¿™äº›å‚æ•°å°†ä¸å‚ä¸è®­ç»ƒ")
    else:
        print("â„¹ï¸  æœªå¯ç”¨å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œæ‰€æœ‰å‚æ•°éƒ½å°†å‚ä¸è®­ç»ƒ")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    return model, tokenizer

def load_fuse_model_tokenizer_llmboost_sharekv(model_name, freeze_first_model):
    """åŠ è½½ llmboost-linear èåˆæ¨¡å‹ï¼ˆä½¿ç”¨åŠ æ³•èåˆï¼‰"""
    try:
        from EnsembleLLM.EnsembleQwen3LLMBOOSTGATE.modeling_qwen3 import QwenBoostGateForCausalLM
        from EnsembleLLM.EnsembleQwen3LLMBOOSTGATE.configuration_qwen3 import Qwen3Config
    except ImportError:
        try:
            from EnsembleQwen3LLMBOOSTGATE.modeling_qwen3 import QwenBoostGateForCausalLM
            from EnsembleQwen3LLMBOOSTGATE.configuration_qwen3 import Qwen3Config
        except ImportError as exc:
            _current_dir = os.path.dirname(os.path.abspath(__file__))
            _project_root = os.path.abspath(os.path.join(_current_dir, ".."))
            if _project_root not in sys.path:
                sys.path.insert(0, _project_root)
            try:
                from EnsembleQwen3LLMBOOSTGATE.modeling_qwen3 import QwenBoostGateForCausalLM
                from EnsembleQwen3LLMBOOSTGATE.configuration_qwen3 import Qwen3Config
            except ImportError:
                raise ImportError("Cannot locate `QwenBoostGateForCausalLM`. Please check PYTHONPATH.") from exc
    
    # ä½¿ç”¨ from_pretrained åŠ è½½
    model = QwenBoostGateForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map=None,
        attn_implementation="flash_attention_2",
        low_cpu_mem_usage=True,
    )
    print(f"âœ… Model (llmboost-sharekv) loaded with dtype: {next(model.parameters()).dtype}")
    # å¦‚æœéœ€è¦å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼ˆsub_models.0ï¼‰ï¼Œåˆ™å°†å…¶æ‰€æœ‰å‚æ•°çš„ requires_grad è®¾ç½®ä¸º False
    # å¤„ç† freeze_first_model å‚æ•°ï¼Œæ”¯æŒ bool ç±»å‹å’Œå­—ç¬¦ä¸²ç±»å‹
    should_freeze = False
    if isinstance(freeze_first_model, bool):
        should_freeze = freeze_first_model
    elif isinstance(freeze_first_model, str):
        should_freeze = freeze_first_model.lower() in ("true", "1", "yes", "on")
    else:
        should_freeze = bool(freeze_first_model)
    
    if should_freeze:
        frozen_count = 0
        for name, param in model.named_parameters():
            if name.startswith("sub_models.0"):
                param.requires_grad = False
                frozen_count += 1
        print(f"âœ… å·²å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ (sub_models.0) çš„ {frozen_count} ä¸ªå‚æ•°ï¼Œè¿™äº›å‚æ•°å°†ä¸å‚ä¸è®­ç»ƒ")
    else:
        print("â„¹ï¸  æœªå¯ç”¨å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œæ‰€æœ‰å‚æ•°éƒ½å°†å‚ä¸è®­ç»ƒ")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    return model, tokenizer

def load_fuse_model_tokenizer_v2(model_name):
    try:
        from EnsembleLLM.EnsembleQwen3_v2.modeling_qwen3 import QwenBoostForCausalLM
    except ImportError:
        try:
            from EnsembleQwen3_v2.modeling_qwen3 import QwenBoostForCausalLM
        except ImportError as exc:
            _current_dir = os.path.dirname(os.path.abspath(__file__))
            _project_root = os.path.abspath(os.path.join(_current_dir, ".."))
            if _project_root not in sys.path:
                sys.path.insert(0, _project_root)
            try:
                from EnsembleQwen3_v2.modeling_qwen3 import QwenBoostForCausalLM
            except ImportError:
                raise ImportError("Cannot locate `Qwen3ForEnsemble`. Please check PYTHONPATH.") from exc
    model = QwenBoostForCausalLM.from_pretrained(
        model_name,
        torch_dtype="auto",
        device_map=None,      # â—â—ä¸è¦è‡ªåŠ¨æ”¾ GPU
        low_cpu_mem_usage=True,
        # attn_implementation="flash_attention_2"
    )

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    return model, tokenizer


def load_fuse_llmboost_model_tokenizer(model_name):
    try:
        from EnsembleLLM.EnsembleQwen3LLMBOOST.modeling_qwen3 import QwenBoostForCausalLM
    except ImportError:
        try:
            from EnsembleQwen3LLMBOOST.modeling_qwen3 import QwenBoostForCausalLM
        except ImportError as exc:
            _current_dir = os.path.dirname(os.path.abspath(__file__))
            _project_root = os.path.abspath(os.path.join(_current_dir, ".."))
            if _project_root not in sys.path:
                sys.path.insert(0, _project_root)
            try:
                from EnsembleQwen3LLMBOOST.modeling_qwen3 import QwenBoostForCausalLM
            except ImportError:
                raise ImportError("Cannot locate `Qwen3ForEnsemble`. Please check PYTHONPATH.") from exc
    model = QwenBoostForCausalLM.from_pretrained(
        model_name,
        torch_dtype="auto",
        device_map=None,      # â—â—ä¸è¦è‡ªåŠ¨æ”¾ GPU
        low_cpu_mem_usage=True,
        # attn_implementation="flash_attention_2"
    )

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    return model, tokenizer
