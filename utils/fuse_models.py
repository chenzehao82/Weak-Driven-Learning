import os
import json
import sys
import torch
import re
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig

# Handle execution both as package (`EnsembleLLM.utils`) and as top-level script (`utils`)
try:
    from EnsembleLLM.EnsembleQwen3.modeling_qwen3 import QwenBoostForCausalLM
except ImportError:
    try:
        from EnsembleQwen3.modeling_qwen3 import QwenBoostForCausalLM
    except ImportError as exc:
        _current_dir = os.path.dirname(os.path.abspath(__file__))
        _project_root = os.path.abspath(os.path.join(_current_dir, ".."))
        if _project_root not in sys.path:
            sys.path.insert(0, _project_root)
        try:
            from EnsembleQwen3.modeling_qwen3 import QwenBoostForCausalLM
        except ImportError:
            raise ImportError("Cannot locate `Qwen3ForEnsemble`. Please check PYTHONPATH.") from exc

def extract_submodel(ensemble_model_path, submodel_idx, save_dir, torch_dtype=torch.bfloat16):
    """
    ä»èåˆæ¨¡å‹ä¸­æå–æŒ‡å®šçš„å­æ¨¡å‹å¹¶ä¿å­˜ä¸ºç‹¬ç«‹æ¨¡å‹ã€‚

    Args:
        ensemble_model_path (str): èåˆæ¨¡å‹è·¯å¾„
        submodel_idx (int): è¦æå–çš„å­æ¨¡å‹ç´¢å¼•ï¼ˆ0è¡¨ç¤ºç¬¬ä¸€ä¸ªï¼Œ1è¡¨ç¤ºç¬¬äºŒä¸ªï¼‰
        save_dir (str): ä¿å­˜çš„ç›®å½•
        torch_dtype: ä¿å­˜çš„æƒé‡ç²¾åº¦ (é»˜è®¤ bfloat16)

    Returns:
        str: ä¿å­˜è·¯å¾„
    """
    print(f"ğŸ”¹ ä»èåˆæ¨¡å‹ä¸­æå–å­æ¨¡å‹:")
    print(f"   - èåˆæ¨¡å‹: {ensemble_model_path}")
    print(f"   - å­æ¨¡å‹ç´¢å¼•: {submodel_idx}")
    print(f"   - ä¿å­˜åˆ°: {save_dir}")

    os.makedirs(save_dir, exist_ok=True)

    # åŠ è½½èåˆæ¨¡å‹
    print("\nğŸ”¹ åŠ è½½èåˆæ¨¡å‹...")
    ensemble_model = QwenBoostForCausalLM.from_pretrained(
        ensemble_model_path,
        torch_dtype="auto",
        device_map=None,
        attn_implementation="flash_attention_2"
    )
    ensemble_state_dict = ensemble_model.state_dict()

    # æå–å­æ¨¡å‹çš„æƒé‡
    submodel_prefix = f"sub_models.{submodel_idx}."
    extracted_state_dict = {}
    
    print(f"\nğŸ”¹ æå– {submodel_prefix} çš„æƒé‡...")
    for key, value in ensemble_state_dict.items():
        if key.startswith(submodel_prefix):
            # ç§»é™¤ sub_models.{idx}. å‰ç¼€
            new_key = key[len(submodel_prefix):]
            # è·³è¿‡ prev_*_proj æƒé‡ï¼ˆè¿™äº›æ˜¯ç”¨äºgateçš„ï¼Œä¸å±äºåŸå§‹æ¨¡å‹ï¼‰
            if "prev_" not in new_key:
                extracted_state_dict[new_key] = value.to(torch_dtype)
    
    print(f"   âœ… æå–äº† {len(extracted_state_dict)} ä¸ªæƒé‡å¼ é‡")

    # ä¿å­˜ tokenizer
    print("\nğŸ”¹ ä¿å­˜ tokenizer ...")
    tokenizer = AutoTokenizer.from_pretrained(ensemble_model_path)
    tokenizer.save_pretrained(save_dir)

    # åŠ è½½åŸºç¡€æ¨¡å‹ç»“æ„ï¼ˆç”¨äºä¿å­˜é…ç½®ï¼‰
    print("\nğŸ”¹ ä¿å­˜ config.json ...")
    base_config = AutoConfig.from_pretrained(ensemble_model_path)
    # ä»èåˆæ¨¡å‹çš„é…ç½®ä¸­è·å–å­æ¨¡å‹é…ç½®
    if hasattr(base_config, "sub_model_cfgs") and base_config.sub_model_cfgs:
        # å¤åˆ¶å­æ¨¡å‹é…ç½®å­—å…¸ï¼ˆé¿å…ä¿®æ”¹åŸå§‹é…ç½®ï¼‰
        submodel_config_dict = dict(base_config.sub_model_cfgs[submodel_idx])
        # ç§»é™¤èåˆç›¸å…³çš„é…ç½®å­—æ®µ
        submodel_config_dict.pop("num_sub_models", None)
        submodel_config_dict.pop("is_llmboost", None)
        submodel_config_dict.pop("sub_model_cfgs", None)
        # æ¢å¤ä¸ºæ ‡å‡†çš„ Qwen3 é…ç½®
        if "architectures" not in submodel_config_dict or not submodel_config_dict.get("architectures"):
            submodel_config_dict["architectures"] = ["Qwen3ForCausalLM"]
    else:
        # å¦‚æœæ²¡æœ‰å­æ¨¡å‹é…ç½®ï¼Œä½¿ç”¨åŸºç¡€é…ç½®
        submodel_config_dict = base_config.to_dict()
        submodel_config_dict.pop("num_sub_models", None)
        submodel_config_dict.pop("is_llmboost", None)
        submodel_config_dict.pop("sub_model_cfgs", None)
        if "architectures" not in submodel_config_dict or not submodel_config_dict.get("architectures"):
            submodel_config_dict["architectures"] = ["Qwen3ForCausalLM"]

    with open(os.path.join(save_dir, "config.json"), "w") as f:
        json.dump(submodel_config_dict, f, indent=2, ensure_ascii=False)

    # ä¿å­˜æƒé‡
    print("\nğŸ”¹ ä¿å­˜æƒé‡...")
    final_path = os.path.join(save_dir, "pytorch_model.bin")
    torch.save(extracted_state_dict, final_path)

    print(f"\nğŸ‰ æå–å®Œæˆ! ä¿å­˜åˆ°: {final_path}")
    return final_path

def fuse_submodels(model_list, save_dir, torch_dtype=torch.bfloat16, fusion_lambda=0.5):
    """
    å°†å¤šä¸ª CausalLM æ¨¡å‹åˆå¹¶ä¸ºä¸€ä¸ª Ensemble æ¨¡å‹çš„æƒé‡æ ¼å¼ã€‚

    Args:
        model_list (List[str]): æ¨¡å‹è·¯å¾„åˆ—è¡¨æˆ– HF åç§°åˆ—è¡¨
        save_dir (str): ä¿å­˜çš„ç›®å½•
        torch_dtype: ä¿å­˜çš„æƒé‡ç²¾åº¦ (é»˜è®¤ bfloat16)
        fusion_lambda (float): èåˆæƒé‡ï¼Œå‰é¢çš„æ¨¡å‹å  (1-lambda)ï¼Œåé¢çš„æ¨¡å‹å  lambdaã€‚é»˜è®¤ 0.5 è¡¨ç¤ºå¹³å‡èåˆ

    Returns:
        str: ä¿å­˜è·¯å¾„
    """

    print(f"ğŸ”¹ Loading {len(model_list)} models:")
    for m in model_list:
        print(f"   - {m}")

    os.makedirs(save_dir, exist_ok=True)

    print("\nğŸ”¹ Saving tokenizer ...")
    tokenizer = AutoTokenizer.from_pretrained(model_list[0])
    tokenizer.save_pretrained(save_dir)

    print("\nğŸ”¹ Loading model weights ...")
    models = [
        AutoModelForCausalLM.from_pretrained(m, torch_dtype=torch_dtype)
        for m in model_list
    ]

    print("\nğŸ”¹ Saving config.json ...")
    config = AutoConfig.from_pretrained(model_list[0])
    config_dict = config.to_dict()

    # ç¡®ä¿ä¿å­˜ model_typeï¼Œè¿™å¯¹äºæ­£ç¡®åŠ è½½æ¨¡å‹å¾ˆé‡è¦
    if "model_type" not in config_dict:
        # å¦‚æœ config ä¸­æ²¡æœ‰ model_typeï¼Œå°è¯•ä»æ¨¡å‹è·¯å¾„æˆ–é…ç½®æ¨æ–­
        if hasattr(config, "model_type"):
            config_dict["model_type"] = config.model_type
        elif "qwen2" in model_list[0].lower():
            config_dict["model_type"] = "qwen2"
        elif "qwen3" in model_list[0].lower():
            config_dict["model_type"] = "qwen3"
        else:
            pass

    sub_model_cfgs = []
    for idx, m in enumerate(model_list):
        sub_cfg = AutoConfig.from_pretrained(m)
        sub_cfg_dict = sub_cfg.to_dict()
        # ç¡®ä¿æ¯ä¸ªå­é…ç½®ä¹Ÿæœ‰ model_type
        if "model_type" not in sub_cfg_dict:
            if hasattr(sub_cfg, "model_type"):
                sub_cfg_dict["model_type"] = sub_cfg.model_type
            elif "qwen2" in m.lower():
                sub_cfg_dict["model_type"] = "qwen2"
            elif "qwen3" in m.lower():
                sub_cfg_dict["model_type"] = "qwen3"
        sub_model_cfgs.append(sub_cfg_dict)
    
    config_dict.update({
        "num_sub_models": len(models),
        # "num_hidden_layers": config.num_hidden_layers * len(models),
        # "layer_types": config.layer_types * len(models),
        "architectures": ["QwenBoostForCausalLM"],   # è®©ä½ çš„ Ensemble ç±»è¢«è¯†åˆ«
        "is_llmboost": False,
        "sub_model_cfgs": sub_model_cfgs,
        "fusion_lambda": fusion_lambda,  # èåˆæƒé‡ï¼šå‰é¢çš„æ¨¡å‹å  (1-lambda)ï¼Œåé¢çš„æ¨¡å‹å  lambda
    })
    
    print(f"ğŸ”¹ ä¿å­˜çš„ model_type: {config_dict.get('model_type', 'æœªè®¾ç½®')}")

    with open(os.path.join(save_dir, "config.json"), "w") as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False)

    merged_state_dict = {}

    for idx, model in enumerate(models):
        sub_prefix = f"sub_models.{idx}"
        print(f"   â¤ Processing {sub_prefix} ...")

        # éå†æ¯ä¸ª weight
        for k, v in model.state_dict().items():
            merged_state_dict[f"{sub_prefix}.{k}"] = v.to(torch_dtype)

        # lm_head ï¼ˆæŸäº›æ¨¡å‹æ²¡æœ‰å•ç‹¬ lm_headï¼‰
        if hasattr(model, "lm_head") and hasattr(model.lm_head, "weight"):
            merged_state_dict[f"{sub_prefix}.lm_head.weight"] = model.lm_head.weight.to(torch_dtype)

    print(f"   âœ… Collected {len(merged_state_dict)} tensors")

    final_path = os.path.join(save_dir, "pytorch_model.bin")
    torch.save(merged_state_dict, final_path)

    print(f"\nğŸ‰ Fusion complete! Saved to: {final_path}")
    return final_path

def load_fuse_model_tokenizer(model_name):
    model = QwenBoostForCausalLM.from_pretrained(
        model_name,
        torch_dtype="auto",
        device_map=None,      # â—â—ä¸è¦è‡ªåŠ¨æ”¾ GPU
        # low_cpu_mem_usage=True,
        attn_implementation="flash_attention_2"
    )

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    return model, tokenizer

def load_fuse_model_tokenizer_vote(model_name, freeze_first_model):
    # åˆ¤æ–­æ˜¯ qwen2 è¿˜æ˜¯ qwen3ï¼šå…ˆå°è¯•åŠ è½½ config æ£€æŸ¥
    is_qwen2 = False
    try:
        config = AutoConfig.from_pretrained(model_name)
        # ä¼˜å…ˆæ£€æŸ¥ config ä¸­çš„ model_type
        if hasattr(config, 'model_type') and config.model_type:
            model_type_str = str(config.model_type).lower()
            is_qwen2 = 'qwen2' in model_type_str
            print(f"ğŸ”¹ ä» config.model_type è¯†åˆ«: {config.model_type} -> {'Qwen2' if is_qwen2 else 'Qwen3'}")
        # å¦‚æœ model_type æ— æ³•ç¡®å®šï¼Œæ£€æŸ¥ hidden_sizeï¼ˆqwen2.5-3b é€šå¸¸æ˜¯ 2048ï¼‰
        if not is_qwen2 and hasattr(config, 'hidden_size'):
            hidden_size = config.hidden_size
            # qwen2.5-3b çš„ hidden_size æ˜¯ 2048ï¼Œqwen3 é€šå¸¸æ˜¯ 2560 æˆ–æ›´å¤§
            if hidden_size == 2048:
                is_qwen2 = True
                print(f"ğŸ”¹ ä» hidden_size={hidden_size} æ¨æ–­ä¸º Qwen2")
            elif hidden_size >= 2560:
                is_qwen2 = False
                print(f"ğŸ”¹ ä» hidden_size={hidden_size} æ¨æ–­ä¸º Qwen3")
        # å¦‚æœè¿˜æ˜¯æ— æ³•ç¡®å®šï¼Œæ£€æŸ¥ architectures
        if not is_qwen2 and hasattr(config, 'architectures') and config.architectures:
            is_qwen2 = any('qwen2' in str(arch).lower() for arch in config.architectures)
            if is_qwen2:
                print(f"ğŸ”¹ ä» architectures è¯†åˆ«ä¸º Qwen2")
        # æœ€åæ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not is_qwen2:
            is_qwen2 = 'qwen2' in model_name.lower()
            if is_qwen2:
                print(f"ğŸ”¹ ä»æ¨¡å‹è·¯å¾„è¯†åˆ«ä¸º Qwen2")
    except Exception as e:
        # å¦‚æœåŠ è½½ config å¤±è´¥ï¼Œæ ¹æ®è·¯å¾„åˆ¤æ–­
        print(f"âš ï¸  åŠ è½½ config å¤±è´¥: {e}ï¼Œä½¿ç”¨è·¯å¾„åˆ¤æ–­")
        is_qwen2 = 'qwen2' in model_name.lower()
    
    # æ ¹æ®åˆ¤æ–­ç»“æœå¯¼å…¥ç›¸åº”çš„æ¨¡å—
    if is_qwen2:
        try:
            from EnsembleLLM.EnsembleQwen2LLMBOOST.modeling_qwen2 import QwenBoostForCausalLM
        except ImportError:
            try:
                from EnsembleQwen2LLMBOOST.modeling_qwen2 import QwenBoostForCausalLM
            except ImportError as exc:
                _current_dir = os.path.dirname(os.path.abspath(__file__))
                _project_root = os.path.abspath(os.path.join(_current_dir, ".."))
                if _project_root not in sys.path:
                    sys.path.insert(0, _project_root)
                try:
                    from EnsembleQwen2LLMBOOST.modeling_qwen2 import QwenBoostForCausalLM
                except ImportError:
                    raise ImportError("Cannot locate `Qwen2ForEnsemble`. Please check PYTHONPATH.") from exc
        print("ğŸ”¹ ä½¿ç”¨ Qwen2 æ¨¡å‹ (vote-base)")
    else:
        try:
            from EnsembleLLM.EnsembleQwen3.modeling_qwen3 import QwenBoostForCausalLM
        except ImportError:
            try:
                from EnsembleQwen3.modeling_qwen3 import QwenBoostForCausalLM
            except ImportError as exc:
                _current_dir = os.path.dirname(os.path.abspath(__file__))
                _project_root = os.path.abspath(os.path.join(_current_dir, ".."))
                if _project_root not in sys.path:
                    sys.path.insert(0, _project_root)
                try:
                    from EnsembleQwen3.modeling_qwen3 import QwenBoostForCausalLM
                except ImportError:
                    raise ImportError("Cannot locate `Qwen3ForEnsemble`. Please check PYTHONPATH.") from exc
        print("ğŸ”¹ ä½¿ç”¨ Qwen3 æ¨¡å‹ (vote-base)")
    
    model = QwenBoostForCausalLM.from_pretrained(
        model_name,
        torch_dtype="auto",
        device_map=None,      # â—â—ä¸è¦è‡ªåŠ¨æ”¾ GPU
        # low_cpu_mem_usage=True,
        attn_implementation="flash_attention_2"
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    # å¦‚æœéœ€è¦å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼ˆsub_models.0ï¼‰ï¼Œåˆ™å°†å…¶æ‰€æœ‰å‚æ•°çš„ requires_grad è®¾ç½®ä¸º False
    # å¤„ç† freeze_first_model å‚æ•°ï¼Œæ”¯æŒ bool ç±»å‹å’Œå­—ç¬¦ä¸²ç±»å‹
    should_freeze = False
    if isinstance(freeze_first_model, bool):
        should_freeze = freeze_first_model
    elif isinstance(freeze_first_model, str):
        should_freeze = freeze_first_model.lower() in ("true", "1", "yes", "on")
    else:
        should_freeze = bool(freeze_first_model)
    
    if should_freeze:
        frozen_count = 0
        for name, param in model.named_parameters():
            if name.startswith("sub_models.0"):
                param.requires_grad = False
                frozen_count += 1
        print(f"âœ… å·²å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ (sub_models.0) çš„ {frozen_count} ä¸ªå‚æ•°ï¼Œè¿™äº›å‚æ•°å°†ä¸å‚ä¸è®­ç»ƒ")
    else:
        print("â„¹ï¸  æœªå¯ç”¨å†»ç»“ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œæ‰€æœ‰å‚æ•°éƒ½å°†å‚ä¸è®­ç»ƒ")
    return model, tokenizer
