#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import argparse
os.environ["HF_HOME"] = "/root/buaa/hf_cache"
os.environ["TRANSFORMERS_OFFLINE"] = "1"

# é¿å…å½“å‰é¡¹ç›®æ ¹ç›®å½•é‡Œçš„æœ¬åœ° `transformers/` åŒ…æŠ¢å åŒå pip åŒ…ï¼Œå¯¼è‡´å¾ªç¯å¯¼å…¥
_this_dir = os.path.dirname(os.path.abspath(__file__))              # .../EnsembleLLM/weights
_project_root = os.path.abspath(os.path.join(_this_dir, "..", ".."))  # /root/buaa/czh
if _project_root in sys.path:
    sys.path.remove(_project_root)

from transformers import AutoModelForCausalLM, AutoTokenizer

def main():
    parser = argparse.ArgumentParser(description="å¤åˆ¶ base æ¨¡å‹åˆ° stage0_m0")
    parser.add_argument("--model-name", type=str, required=True, help="HuggingFace æ¨¡å‹åç§°ï¼Œå¦‚ Qwen/Qwen3-8B-Base")
    parser.add_argument("--output-dir", type=str, required=True, help="è¾“å‡ºç›®å½•ï¼Œå¦‚ /root/buaa/czh/weights/ensemble/Qwen3-8B-Base")
    args = parser.parse_args()
    
    model_name = args.model_name
    target_dir = os.path.join(args.output_dir, "stage0_m0")

    os.makedirs(target_dir, exist_ok=True)
    print(f"ğŸŒ ä» HuggingFace åŠ è½½æ¨¡å‹: {model_name}")

    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        torch_dtype="auto",
        device_map="cpu",
    )

    print(f"ğŸ’¾ ä¿å­˜åˆ°: {target_dir}")
    tokenizer.save_pretrained(target_dir)
    model.save_pretrained(target_dir)

    print(f"âœ… å®Œæˆï¼š{model_name} å·²å¤åˆ¶åˆ° {target_dir}")

if __name__ == "__main__":
    main()