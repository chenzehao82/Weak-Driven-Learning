#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
os.environ["HF_HOME"] = "/root/buaa/hf_cache"
os.environ["TRANSFORMERS_OFFLINE"] = "1"

# é¿å…å½“å‰é¡¹ç›®æ ¹ç›®å½•é‡Œçš„æœ¬åœ° `transformers/` åŒ…æŠ¢å åŒå pip åŒ…ï¼Œå¯¼è‡´å¾ªç¯å¯¼å…¥
_this_dir = os.path.dirname(os.path.abspath(__file__))              # .../EnsembleLLM/weights
_project_root = os.path.abspath(os.path.join(_this_dir, "..", ".."))  # /root/buaa/czh
if _project_root in sys.path:
    sys.path.remove(_project_root)

from transformers import AutoModelForCausalLM, AutoTokenizer

def main():
    model_name = "Qwen/Qwen3-4B-Base"
    # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼ŒåŸºäºè„šæœ¬æ‰€åœ¨ä½ç½®
    _current_dir = os.path.dirname(os.path.abspath(__file__))
    _project_root = os.path.dirname(_current_dir)
    target_dir = os.path.join(_project_root, "weights/ensemble/Qwen3-4B-Base/stage0_m0")

    os.makedirs(target_dir, exist_ok=True)
    print(f"ğŸŒ ä» HuggingFace åŠ è½½æ¨¡å‹: {model_name}")

    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        torch_dtype="auto",
        device_map="cpu",
    )

    print(f"ğŸ’¾ ä¿å­˜åˆ°: {target_dir}")
    tokenizer.save_pretrained(target_dir)
    model.save_pretrained(target_dir)

    print("âœ… å®Œæˆï¼šQwen3-4B-Base å·²å¤åˆ¶åˆ° stage0_m0")

if __name__ == "__main__":
    main()