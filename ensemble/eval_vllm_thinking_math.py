#!/usr/bin/env python
# -*- coding: utf-8 -*-
# import multiprocessing as mp
# mp.set_start_method("spawn", force=True)
import argparse, json, os, re, time, random, sys
from typing import List, Dict, Any, Optional
import numpy as np
import pandas as pd
import re
from pathlib import Path

# Add parent directory to path to import utils
_current_dir = Path(__file__).parent
_parent_dir = _current_dir.parent
if str(_parent_dir) not in sys.path:
    sys.path.insert(0, str(_parent_dir))

# -----------------------------
# Import Latex verification dependencies
# -----------------------------
from latex2sympy2_extended import NormalizationConfig
from math_verify import LatexExtractionConfig, parse, verify
import datetime
from utils.prompts import SYSTEM_PROMPT

PREFIX_CHECKPOINT_DIR = "checkpoint"
_re_checkpoint = re.compile(r"^" + PREFIX_CHECKPOINT_DIR + r"\-(\d+)$")
def get_last_checkpoint(folder):
    content = os.listdir(folder)
    checkpoints = [
        path
        for path in content
        if _re_checkpoint.search(path) is not None and os.path.isdir(os.path.join(folder, path))
    ]
    if len(checkpoints) == 0:
        return
    return os.path.join(folder, max(checkpoints, key=lambda x: int(_re_checkpoint.search(x).groups()[0])))


BOXED_PAT = re.compile(r"\\boxed\{([^{}]+)\}")
INT_PAT   = re.compile(r"(-?\d+)")

def apply_chat_template(tokenizer, question, thinking: bool = True) -> str:
    text = tokenizer.apply_chat_template(
        conversation=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": question}
        ],
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=thinking
    )
    return text


def load_general_dataset(path: str) -> List[Dict[str, Any]]:
    data = []
    if path.endswith(".jsonl"):
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line))
    elif path.endswith(".json"):
        with open(path, "r", encoding="utf-8") as f:
            obj = json.load(f)
            if isinstance(obj, list):
                data = obj
            elif isinstance(obj, dict) and "data" in obj:
                data = obj["data"]
            else:
                raise ValueError("JSON format not recognized.")
    else:
        raise ValueError("dataset must be .json or .jsonl")

    norm = []
    for i, ex in enumerate(data):
        # å°è¯•å¤šä¸ªå¯èƒ½çš„é—®é¢˜å­—æ®µå
        q = ex.get("question") or ex.get("instruction") or ex.get("problem") or ex.get("prompt") or ex.get("input")
        # å°è¯•å¤šä¸ªå¯èƒ½çš„ç­”æ¡ˆå­—æ®µå
        a = ex.get("answer") or ex.get("label") or ex.get("solution") or ex.get("output")
        if q is None or a is None:
            continue
        norm.append({"id": ex.get("id", i), "question": q, "answer": str(a).strip()})
    return norm

# -----------------------------
# ç®€å•æ•°å­—/å­—æ¯æå–æ–¹å¼ï¼ˆç”¨äºç®€å•æ•°å­¦æ•°æ®é›†ï¼‰
# -----------------------------
def extract_answer_number(sentence: str) -> float:
    """ä»æ–‡æœ¬ä¸­æå–æ•°å­—ç­”æ¡ˆï¼ˆå–æœ€åä¸€ä¸ªæ•°å­—ï¼‰"""
    sentence = sentence.replace(',', '')
    pred = [s for s in re.findall(r'-?\d+\.?\d*', sentence)]
    if not pred:
        return float('inf')
    try:
        pred_answer = float(pred[-1])
    except ValueError:
        pred_answer = float('inf')
    return pred_answer


def extract_answer_letter(sentence: str) -> str:
    """ä»æ–‡æœ¬ä¸­æå–å­—æ¯ç­”æ¡ˆï¼ˆA-Eï¼‰"""
    sentence_ = sentence.strip()
    pred_answers = re.findall(r'A|B|C|D|E', sentence_)
    if pred_answers:
        return pred_answers[-1]
    else:
        return ''


def verify_simple_number(pred_text: str, gold: str, miss: float = 1e-3) -> Optional[bool]:
    """ç®€å•æ•°å­—éªŒè¯"""
    try:
        # å°è¯•å°† gold è½¬ä¸º float
        try:
            gold_num = float(gold)
        except ValueError:
            return None
        
        # æå–é¢„æµ‹ç­”æ¡ˆ
        pred_num = extract_answer_number(pred_text)
        
        if pred_num == float('inf'):
            return False
        
        # æ¯”è¾ƒ
        return abs(gold_num - pred_num) <= miss
    except Exception as e:
        print(f"âŒ Simple number verification failed: {e}")
        return None


def verify_simple_letter(pred_text: str, gold: str) -> Optional[bool]:
    """ç®€å•å­—æ¯éªŒè¯"""
    try:
        pred_letter = extract_answer_letter(pred_text)
        if not pred_letter:
            return False
        return pred_letter == gold.strip().upper()
    except Exception as e:
        print(f"âŒ Simple letter verification failed: {e}")
        return None

# -----------------------------
# Latex-based Verificationï¼ˆç”¨äºå¤æ‚æ•°å­¦æ•°æ®é›†ï¼‰
# -----------------------------
def verify_with_latex(idx: str, pred: str, gold: str) -> Optional[bool]:
    try:
        gold = '$' + gold.strip().strip('$') + '$'
        gold_parsed = parse(
            gold,
            extraction_mode="first_match",
        )
        if len(gold_parsed) == 0:
            print(f"âš ï¸ Failed to parse gold solution: {gold}")
            return None

        answer_parsed = parse(
            pred,
            extraction_config=[
                LatexExtractionConfig(
                    normalization_config=NormalizationConfig(
                        nits=False,
                        malformed_operators=False,
                        basic_latex=True,
                        equations=True,
                        boxed="all",
                        units=True,
                    ),
                    boxed_match_priority=0,
                    try_extract_without_anchor=False,
                )
            ],
            extraction_mode="all",
        )
        print(f"id: {idx}, pred_parsed: {answer_parsed}, gold_parsed: {gold_parsed}, gold: {gold}")
        return bool(verify(gold_parsed, answer_parsed))
    except Exception as e:
        print(f"âŒ verify failed: {e}\n id={idx}\n pred={pred}\n gold={gold}")
        return None


def get_verification_method(dataset_name: str):
    """æ ¹æ®æ•°æ®é›†åç§°è¿”å›å¯¹åº”çš„éªŒè¯æ–¹æ³•"""
    dataset_name = dataset_name.lower()
    
    # ç®€å•æ•°å­—æå–çš„æ•°æ®é›†
    simple_number_datasets = [
        "multiarith", "addsub", "singleeq", "gsm8k", "svamp", "mawps", 
        "math_10k_500", "math_10k_small_5", "math_10k"
    ]
    
    # å­—æ¯é€‰æ‹©çš„æ•°æ®é›†
    letter_datasets = ["aqua"]
    
    # LaTeX éªŒè¯çš„æ•°æ®é›†
    latex_datasets = ["aime", "aime2025", "math500", "math"]
    
    if any(ds in dataset_name for ds in simple_number_datasets):
        return "simple_number"
    elif any(ds in dataset_name for ds in letter_datasets):
        return "simple_letter"
    elif any(ds in dataset_name for ds in latex_datasets):
        return "latex"
    else:
        # é»˜è®¤ä½¿ç”¨ LaTeX éªŒè¯
        print(f"âš ï¸ Unknown dataset '{dataset_name}', defaulting to LaTeX verification")
        return "latex"


def should_use_chat_template(dataset_name: str) -> bool:
    """
    æ ¹æ®æ•°æ®é›†åç§°åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨ chat_template
    ç®€å•æ•°æ®é›†ï¼ˆå¦‚ math_10k, gsm8k ç­‰ï¼‰ä¸éœ€è¦ chat_template
    å¤æ‚æ•°æ®é›†ï¼ˆå¦‚ aime, math500ï¼‰éœ€è¦ chat_template
    """
    dataset_name = dataset_name.lower()
    
    # ä¸éœ€è¦ chat_template çš„æ•°æ®é›†ï¼ˆé€šå¸¸æ˜¯ç®€å•çš„æ•°å­¦é—®é¢˜ï¼‰
    simple_datasets = [
        "multiarith", "addsub", "singleeq", "gsm8k", "svamp", "mawps", 
        "math_10k", "aqua"
    ]
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ç®€å•æ•°æ®é›†
    if any(ds in dataset_name for ds in simple_datasets):
        return False
    
    # å…¶ä»–æ•°æ®é›†ï¼ˆaime, math500 ç­‰ï¼‰ä½¿ç”¨ chat_template
    return True

# -----------------------------
# VLLM Backend
# -----------------------------
class VLLMBackend:
    def __init__(self, model, tensor_parallel_size=1, max_model_len=8192, dtype=None, gpu_memory_utilization=0.9, seed=None):
        from vllm import LLM, SamplingParams
        self.LLM = LLM(model=model, 
                        tensor_parallel_size=tensor_parallel_size, 
                        max_model_len=max_model_len, 
                        download_dir="/root/buaa/hf_cache",
                        gpu_memory_utilization=gpu_memory_utilization,
                        seed=seed)
        self.sp = SamplingParams(temperature=0.5, top_p=0.95, max_tokens=max_model_len, stop=None, seed=seed)

    def generate(self, prompts: List[str]) -> List[str]:
        outs = self.LLM.generate(prompts, self.sp)
        return [o.outputs[0].text for o in outs]

# -----------------------------
# Evaluator
# -----------------------------
def evaluate(dataset_path: str,
             model_path: str,
             tp: int = 1,
             out_csv: str = "results/eval_detail.csv",
             out_json: str = "results/eval_summary.json",
             max_model_len: int = 1024,
             thinking: bool = True,
             repeat: int = 1,
             dataset_name: str = None,
             seed: int = None):

    samples = load_general_dataset(dataset_path)

    # ===== ç¡®å®šæ•°æ®é›†åç§° =====
    if dataset_name is None:
        # ä»è·¯å¾„ä¸­æå–æ•°æ®é›†åç§°
        dataset_name = dataset_path.split("/")[-2] if "/" in dataset_path else "unknown"
    
    # ç»Ÿä¸€ï¼šå§‹ç»ˆä½¿ç”¨ chat_template + LaTeX éªŒè¯
    use_chat_template = True
    verification_method = "latex"
    print(f"ğŸ“Š Dataset: {dataset_name}, Verification method: {verification_method}")
    print(f"ğŸ”§ Use chat_template: {use_chat_template}")

    # ===== æ‰©å……é‡å¤æ ·æœ¬ =====
    expanded_samples = []
    for ex in samples:
        for k in range(repeat):
            ex_copy = ex.copy()
            ex_copy["id"] = f"{ex['id']}_rep{k+1}"
            expanded_samples.append(ex_copy)
    print(f"Loaded {len(samples)} samples â†’ expanded to {len(expanded_samples)} (repeat={repeat})")

    
    engine = VLLMBackend(model_path, tensor_parallel_size=tp, max_model_len=max_model_len, seed=seed)
    tokenizer = engine.LLM.get_tokenizer()

    # å§‹ç»ˆä½¿ç”¨ chat_template æ„é€ å¯¹è¯å¼ prompt
    prompts = [apply_chat_template(tokenizer, question=s["question"], thinking=thinking) for s in expanded_samples]
    
    print(f"ç¤ºä¾‹ Prompt: {prompts[0]}...")
    # exit(0)
    B = 500
    preds, latencies = [], []
    for i in range(0, len(prompts), B):
        batch = prompts[i:i+B]
        t0 = time.time()
        texts = engine.generate(batch)
        dt = time.time() - t0
        per_item = dt / len(batch)
        latencies.extend([per_item] * len(batch))
        preds.extend(texts)

    # ========== è¯„ä¼°ï¼ˆç»Ÿä¸€ä½¿ç”¨ LaTeX éªŒè¯ï¼‰ ==========
    rows, correct = [], 0
    for ex, text, lat in zip(expanded_samples, preds, latencies):
        verified = verify_with_latex(ex["id"], text, ex["answer"])
        
        is_ok = (verified is True)
        correct += int(is_ok)
        rows.append({
            "id": ex["id"],
            "orig_id": ex["id"].split("_rep")[0],   # â­ åŸå§‹æ ·æœ¬ idï¼ˆé‡è¦ï¼‰
            "gt": ex["answer"],
            "pred_text": text.strip(),
            "verified": verified,
            "correct": int(is_ok),
            "latency_s": round(lat, 4),
            "question": ex["question"][:2000],
            "verification_method": verification_method,
        })

    # ====== Acc (sample-level, considers each repetition independently) ======
    total = len(expanded_samples)
    acc = correct / total

    # ====== Pass@N (original-sample-level) ======
    # ä¾‹å¦‚ repeat=10 æ—¶ï¼Œå°±æ˜¯ pass@10
    grouped = {}
    for r in rows:
        oid = r["orig_id"]
        grouped.setdefault(oid, []).append(r["correct"])

    pass_total = len(grouped)  # åŸå§‹æ ·æœ¬æ•°
    pass_success = sum(1 for oid, corr_list in grouped.items() if any(corr_list))
    pass_at_k = pass_success / pass_total if pass_total > 0 else 0.0

    print(f"Pass@{repeat}: {pass_at_k:.4f} ({pass_success}/{pass_total})")

    # ====== ä¿å­˜ CSV ======
    os.makedirs(os.path.dirname(out_csv) or ".", exist_ok=True)
    df = pd.DataFrame(rows)
    df.to_csv(out_csv, index=False, encoding="utf-8")

    # ====== ä¿å­˜ JSON ======
    summary = {
        "acc": acc,
        "correct": correct,
        "total": total,
        "repeat": repeat,
        "pass_at_k": pass_at_k,
        "pass_success": pass_success,
        "orig_total": pass_total,
        "dataset_path": dataset_path,
        "dataset_name": dataset_name,
        "verification_method": verification_method,
        "use_chat_template": use_chat_template,
        "model_path": model_path,
        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "csv_file": out_csv,
        "seed": seed,
    }

    os.makedirs(os.path.dirname(out_json) or ".", exist_ok=True)
    with open(out_json, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)

    # ====== æ—¥å¿— ======
    print(f"\n========== Evaluation ==========")
    print(f"Dataset: {dataset_path}")
    print(f"Dataset Name: {dataset_name}")
    print(f"Verification: {verification_method}")
    print(f"Chat Template: {use_chat_template}")
    print(f"Model:   {model_path}")
    print(f"Seed:    {seed}")
    print(f"Samples: {len(samples)} Ã— {repeat} = {total}")
    print(f"Acc:     {acc:.4f}  ({correct}/{total})")
    print(f"Pass@{repeat}: {pass_at_k:.4f}  ({pass_success}/{pass_total})")
    print(f"Detail CSV saved to: {out_csv}")
    print(f"Summary JSON saved to: {out_json}")

    return acc

# -----------------------------
# CLI
# -----------------------------
def main():
    p = argparse.ArgumentParser()
    p.add_argument("--dataset", default="/root/buaa/czh/BoostLLM/Decoder_Only/dataset/math500/test.jsonl", type=str)
    p.add_argument("--model", default="/root/buaa/czh/Weak-Driving Learning/weights/ensemble/Qwen3-4B-Base/stage1_m1", type=str)
    p.add_argument("--tp", type=int, default=8)
    p.add_argument("--max_model_len", type=int, default=1024 * 4)
    p.add_argument("--thinking", type=bool, default=False)
    p.add_argument("--repeat", type=int, default=3, help="é‡å¤æµ‹è¯•æ¬¡æ•°")
    p.add_argument("--epoch", type=str, default=None, help="è®­ç»ƒè½®æ•°")
    p.add_argument("--dataset_name", type=str, default=None, help="æ•°æ®é›†åç§°ï¼ˆç”¨äºä¿å­˜è·¯å¾„å’Œæ—¥å¿—ï¼‰")
    p.add_argument("--seed", type=int, default=42, help="éšæœºç§å­ï¼ˆç”¨äºå¯å¤ç°æ€§ï¼‰")
    args = p.parse_args()

     # === è‡ªåŠ¨åŠ è½½æœ€æ–° checkpoint ===
    model_path = args.model.rstrip('/')
    # æ£€æŸ¥è·¯å¾„æœ¬èº«æ˜¯å¦å·²ç»æ˜¯checkpointç›®å½•
    basename = os.path.basename(model_path)
    if _re_checkpoint.match(basename):
        # è·¯å¾„æœ¬èº«å°±æ˜¯ä¸€ä¸ªcheckpointç›®å½•ï¼Œç›´æ¥ä½¿ç”¨
        print(f"[Using checkpoint path] {model_path}")
    elif os.path.isdir(model_path):
        # è·¯å¾„æ˜¯ç›®å½•ä½†ä¸æ˜¯checkpointï¼Œå°è¯•æŸ¥æ‰¾checkpoint
        last_ckpt = get_last_checkpoint(model_path)
        if last_ckpt is not None:
            print(f"[Auto-Load] Found latest checkpoint: {last_ckpt}")
            model_path = last_ckpt
        else:
            print(f"[Warning] No checkpoint-* found under {model_path}, using folder as model.")
    else:
        print(f"[Using model path] {model_path}")
    print(f"[Final model path] {model_path}")
    
    # ä»dataseté‡Œæå–dataset_name
    if args.dataset_name is None:
        dataset_name = args.dataset.split("/")[-2]
    else:
        dataset_name = args.dataset_name
    
    # ä»æ¨¡å‹è·¯å¾„ä¸­æå–ç›¸å¯¹è·¯å¾„ï¼ˆbaseline/qwen2.5-7b/checkpoint-872ï¼‰
    # æŸ¥æ‰¾ weights/ æˆ– EnsembleLLM/weights/ åœ¨è·¯å¾„ä¸­çš„ä½ç½®
    if "weights/" in model_path:
        # æå– weights/ ä¹‹åçš„éƒ¨åˆ†
        relative_path = model_path.split("weights/", 1)[1]
    elif "EnsembleLLM/weights/" in model_path:
        # æå– EnsembleLLM/weights/ ä¹‹åçš„éƒ¨åˆ†
        relative_path = model_path.split("/EnsembleLLM/weights/", 1)[1]
    else:
        # å¦‚æœæ‰¾ä¸åˆ°weightsç›®å½•ï¼Œä½¿ç”¨checkpointåç§°ä½œä¸ºç›¸å¯¹è·¯å¾„
        relative_path = basename
    
    # ç»“æœä¿å­˜åœ¨resultsç›®å½•ä¸‹ï¼Œä¿æŒç›¸å¯¹è·¯å¾„ç»“æ„
    out_dir = os.path.join("results", relative_path, str(args.epoch), dataset_name)
    os.makedirs(out_dir, exist_ok=True)
    args.out_csv = f"{out_dir}/eval_detail.csv"
    args.out_json = f"{out_dir}/eval_summary.json"
    print(f"[Final output directory] {out_dir}")
    print(f"[Final output CSV] {args.out_csv}")
    print(f"[Final output JSON] {args.out_json}")

    evaluate(
        dataset_path=args.dataset,
        model_path=model_path,
        tp=args.tp,
        out_csv=args.out_csv,
        out_json=args.out_json,
        max_model_len=args.max_model_len,
        thinking=args.thinking,
        repeat=args.repeat,
        dataset_name=dataset_name,
        seed=args.seed,
    )

if __name__ == "__main__":
    main()
